{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeanlucjackson/w266_final_project/blob/main/code/sandboxes/RR/awesome_T5_pt_inference_triviaqa_sliced.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hh-4vVXTMhpC"
      },
      "source": [
        "### Generate Predictions From An Awesome Validation Dataset\n",
        "\n",
        "This notebook assumes a T5 PyTorch model.\n",
        "\n",
        "Setting the constants in the next call should be all that is necessary to run the validation set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rlCLzceMMahp"
      },
      "outputs": [],
      "source": [
        "# Set these constants for each model and validation dataset combination\n",
        "\n",
        "model_name = \"T5_base_pt_long.quac\"\n",
        "validation_dataset_names = [\"v1\", \"v2\", \"d1\"]\n",
        "\n",
        "save_predictions = True\n",
        "save_mode = 'w' # w for write, a for append\n",
        "\n",
        "max_input_length = 1024\n",
        "\n",
        "num_beams = 4\n",
        "no_repeat_ngram_size = 3\n",
        "max_target_length = 50\n",
        "min_target_length = 1\n",
        "early_stopping = True\n",
        "\n",
        "batch_size = 25\n",
        "\n",
        "start_sample = None  # If None, then 0 will be used\n",
        "end_sample = None # If None, then the end of the set will be used"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJ8lW8X8NJ9o"
      },
      "source": [
        "### Generate Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "c3906UT0dyke"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "vBX23WXdd0v-"
      },
      "outputs": [],
      "source": [
        "!pip install -q sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "17N1XaF7d6dW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5dbec4fa-b6ec-4a05-b793-b5e1af01f6b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "3jvE9TJRd4cG"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "from google.colab import data_table\n",
        "data_table.enable_dataframe_formatter()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "iMHoUiDnd9He"
      },
      "outputs": [],
      "source": [
        "# Some important file locations and constants\n",
        "\n",
        "project_root = \"/content/drive/MyDrive/w266 NLP Final Project/\"\n",
        "#project_root = \"/home/localadmin/Documents/w266_NLP_Final_Project/\"\n",
        "dataset_prefix = project_root + \"Data/triviaqa/valid_pairs_\"\n",
        "model_root = project_root + \"Models/\"\n",
        "prediction_folder = project_root + \"Predictions_3/checkpoint/\"\n",
        "\n",
        "tokenizer = \"google/t5-v1_1-base\"\n",
        "model_folder = model_root + model_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "fEhqOalVycI9"
      },
      "outputs": [],
      "source": [
        "# Get the model and tokenizer\n",
        "\n",
        "T5_tokenizer = T5Tokenizer.from_pretrained(tokenizer)\n",
        "T5_model = T5ForConditionalGeneration.from_pretrained(model_folder)\n",
        "T5_model.to(torch.device('cuda:0'))\n",
        "pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "zYAIRZMxwXNn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85ddcdbf-738d-42e0-a6a7-c0e5042ca091"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/w266 NLP Final Project/Data/triviaqa/valid_pairs_v1.csv\n",
            "Generating predictions using v1 from 0 to 493:\n",
            "25 50 75 100 125 150 175 200 225 250 275 300 325 350 375 400 425 450 475 493 \n",
            "Predictions generated.\n",
            "Write: /content/drive/MyDrive/w266 NLP Final Project/Predictions_3/checkpoint/predictions.T5_base_pt_long.quac.triviaqa_v1.csv\n",
            "/content/drive/MyDrive/w266 NLP Final Project/Data/triviaqa/valid_pairs_v2.csv\n",
            "Generating predictions using v2 from 0 to 492:\n",
            "25 50 75 100 125 150 175 200 225 250 275 300 325 350 375 400 425 450 475 492 \n",
            "Predictions generated.\n",
            "Write: /content/drive/MyDrive/w266 NLP Final Project/Predictions_3/checkpoint/predictions.T5_base_pt_long.quac.triviaqa_v2.csv\n",
            "/content/drive/MyDrive/w266 NLP Final Project/Data/triviaqa/valid_pairs_d1.csv\n",
            "Generating predictions using d1 from 0 to 985:\n",
            "25 50 75 100 125 150 175 200 225 250 275 300 325 350 375 400 425 450 475 500 525 550 575 600 625 650 675 700 725 750 775 800 825 850 875 900 925 950 975 985 \n",
            "Predictions generated.\n",
            "Write: /content/drive/MyDrive/w266 NLP Final Project/Predictions_3/checkpoint/predictions.T5_base_pt_long.quac.triviaqa_d1.csv\n"
          ]
        }
      ],
      "source": [
        "for dataset_name in validation_dataset_names:\n",
        "#  if dataset_name == \"squad\":\n",
        "#    validation_data_file = f\"{dataset_root}squad.hf/valid_pairs.csv\"\n",
        "#  else:  \n",
        "#    validation_data_file = f\"{dataset_root}{dataset_name}/valid_pairs.csv\"\n",
        "  validation_data_file = f\"{dataset_prefix}{dataset_name}.csv\"\n",
        "  print(validation_data_file)\n",
        "  validation_df = pd.read_csv(validation_data_file)\n",
        "  prediction_file = f\"{prediction_folder}predictions.{model_name}.triviaqa_{dataset_name}.csv\" \n",
        "  \n",
        "  start_sample = None\n",
        "  end_sample = None\n",
        "\n",
        "  predictions = []\n",
        "  \n",
        "  if start_sample is None: start_sample = 0\n",
        "  if end_sample is None: end_sample = validation_df.shape[0]\n",
        "  \n",
        "  print(f\"Generating predictions using {dataset_name} from {start_sample} to {end_sample}:\")\n",
        "  for start in range (start_sample, end_sample, batch_size):\n",
        "    to = min([end_sample, start + batch_size])\n",
        "    inputs = T5_tokenizer(validation_df['orig'][start:to].to_list(), return_tensors='pt', max_length=max_input_length, truncation=True, padding=True)\n",
        "    output_ids = T5_model.generate(inputs['input_ids'].cuda(),\n",
        "                                   min_length=min_target_length,\n",
        "                                   max_length=max_target_length,\n",
        "                                   num_beams=num_beams, \n",
        "                                   no_repeat_ngram_size=no_repeat_ngram_size, \n",
        "                                   early_stopping=early_stopping)  \n",
        "    prediction_batch = T5_tokenizer.batch_decode(output_ids, skip_special_tokens=True)   \n",
        "    predictions.extend(prediction_batch)\n",
        "    print (f\"{to} \", end=\"\")\n",
        "    if to%1000 == 0: print()\n",
        "  print(\"\\nPredictions generated.\")\n",
        "\n",
        "  df=pd.DataFrame()\n",
        "  df['context'] = [str.split('context: ')[1] for str in validation_df['orig'][start_sample:end_sample]]\n",
        "  df['answer'] =  [str.split('context: ')[0][26: ] for str in validation_df['orig'][start_sample:end_sample]]\n",
        "  df['target'] = validation_df['target']\n",
        "  df['prediction'] = predictions\n",
        "\n",
        "  if save_predictions:\n",
        "    df.to_csv(prediction_file, mode=save_mode)\n",
        "    print(f\"Write: {prediction_file}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python [conda env:gpu]",
      "language": "python",
      "name": "conda-env-gpu-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}