{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "ciKTbDAo6YZs",
        "qiB1BdLHFV9M",
        "s6XRvkI33MMR",
        "7vP_qHhY09ZX",
        "1OfGhg_snQN5",
        "Mcd_R5JUGCxV"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeanlucjackson/w266_final_project/blob/main/code/sandboxes/RR/rr_evaluation_sandbox.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup"
      ],
      "metadata": {
        "id": "1szttaXlupSn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h2g69HkZJSvo",
        "outputId": "bad00d41-c481-4676-e3a8-ab49784cf581"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "import seaborn as sns\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "import scipy.stats as st\n",
        "\n",
        "from pprint import pprint\n",
        "\n",
        "pd.set_option(\"precision\", 5)\n",
        "sns.set_theme()"
      ],
      "metadata": {
        "id": "Os8PNswbKW-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation_root = \"/content/drive/MyDrive/w266 NLP Final Project/Evaluation/\"\n",
        "filename = evaluation_root + \"evaluation_database.json\"\n",
        "\n",
        "evaluation_df = pd.read_json(filename)"
      ],
      "metadata": {
        "id": "g9knLivdJXSg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation_df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xv_nGZ_gY__4",
        "outputId": "efb09042-b705-417e-88d1-7c23aa39b431"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['prediction_set', 'base_model', 'trained_on', 'tested_on', 'nickname',\n",
              "       'hyperparameter', 'target', 'prediction', 'bleu', 'rouge1', 'rouge2',\n",
              "       'rougeL', 'rougeLsum', 'meteor', 'bertscore-precision',\n",
              "       'bertscore-recall', 'bertscore-f1', 'use', 'bleurt'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation_exclusions = [\n",
        "    \"rouge1\",\n",
        "    \"rouge2\",\n",
        "    \"rougeLsum\",\n",
        "    \"bertscore-precision\",\n",
        "    \"bertscore-recall\",\n",
        "    \"bleurt\",\n",
        "]\n",
        "evaluation_df = evaluation_df.drop(columns=evaluation_exclusions)"
      ],
      "metadata": {
        "id": "N6T0fJ2Xzuhd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation_metrics = evaluation_df.select_dtypes(exclude=\"object\").columns\n",
        "evaluation_metrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P6nfOkkGBxcv",
        "outputId": "b3d0579c-7a45-4400-d6a9-30e37a1bcb04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['bleu', 'rougeL', 'meteor', 'bertscore-f1', 'use'], dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply this mask to remove all samples from non default inference sets\n",
        "default_hyperparameter_mask = [\n",
        "    sample[\"defaults\"] for sample in evaluation_df.hyperparameter\n",
        "]"
      ],
      "metadata": {
        "id": "doiZOH-sqNc9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BART vs T5\n",
        "\n",
        "In the end, it is hard to distinguish BART from T5, with a slight edge going to T5.\n",
        "\n"
      ],
      "metadata": {
        "id": "ciKTbDAo6YZs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We built 5 BART models and 5 T5 models.\n",
        "\n",
        "Each set of 5 models includes one model trained on each of our datasets plus one trained on all of our datasets.\n",
        "\n",
        "We ran the validation sets from each of our datasets on each of our ten models yielding 40 sets of predictions."
      ],
      "metadata": {
        "id": "szRDWDak6v-r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### In The Aggregate\n",
        "\n",
        "We group the prediction sets by base model and calculate the mean for each metric.\n",
        "\n",
        "In every case, the BART models outperform the T5 models.\n",
        "\n",
        "We applied Welch's t-test to the metric distributions between the two models and, in each case, found $p$ values of less than .05 and reject the hypothesis that the distributions have equal means."
      ],
      "metadata": {
        "id": "CBwU-3oYFc2E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = evaluation_df[default_hyperparameter_mask]\n",
        "df.groupby([\"base_model\"]).mean()\n",
        "df_T5 = df[df[\"base_model\"] == \"T5\"]\n",
        "df_BART = df[df[\"base_model\"] == \"bart\"]"
      ],
      "metadata": {
        "id": "0M1zHZUE1Jxv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"For All Predictions Made\")\n",
        "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
        "for ax, metric in zip(axes.flat, evaluation_metrics):\n",
        "    sns.histplot(x=metric, data=df, hue=\"base_model\", ax=ax)\n",
        "list(axes.flat)[-1].remove()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "To8Zu5PvWoED",
        "outputId": "c1e79b59-b85d-4013-9ac5-047f4f487836"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For All Predictions Made\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reject_count = 0\n",
        "fail_to_reject_count = 0\n",
        "\n",
        "print(\"Reject the null hypothesis except where noted otherwise\\n\")\n",
        "\n",
        "for metric in evaluation_metrics:\n",
        "\n",
        "    p = st.ttest_ind(df_T5[metric], df_BART[metric], equal_var=False)[1]\n",
        "\n",
        "    if p <= 0.05:\n",
        "        reject_null = \"\"\n",
        "        reject_count += 1\n",
        "    else:\n",
        "        reject_null = \"***FAIL TO REJECT THE NULL***\"\n",
        "        fail_to_reject_count += 1\n",
        "\n",
        "    print(f\"metric: {metric} Welch's t-test p value {p}, {reject_null}\")\n",
        "\n",
        "print(\n",
        "    f\"\\nreject the null: {reject_count}, fail to reject the null:  {fail_to_reject_count}\"\n",
        ")"
      ],
      "metadata": {
        "id": "YtDlQW4R9vrj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bart_wins = 0\n",
        "t5_wins = 0\n",
        "\n",
        "print(\"BART wins the comparison except where noted otherwise\\n\")\n",
        "\n",
        "for metric in evaluation_metrics:\n",
        "    print(f\"metric: {metric}\", end=\" \")\n",
        "    T5 = df_T5[metric].mean()\n",
        "    BART = df_BART[metric].mean()\n",
        "\n",
        "    if BART > T5:\n",
        "        print(\"\")\n",
        "        bart_wins += 1\n",
        "    else:\n",
        "        print(\"\\t***T5 wins***\")\n",
        "        t5_wins += 1\n",
        "\n",
        "print()\n",
        "print(f\"BART wins: {bart_wins} T5 wins: {t5_wins}\")"
      ],
      "metadata": {
        "id": "zUzMDWDxrbW4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Per Training Set\n",
        "\n",
        "We group the prediction sets by base model and training set and calculate the mean for each metric.\n",
        "\n",
        "We have five metrics and five training sets (four individual plus amalgam).\n",
        "\n",
        "In 13 of those 25 cases, the differences in the distributions are not statistically significant.\n",
        "\n",
        "In the 12 cases where the differences in the distributions are significant, BART outperforms T5 for all 5 metrics on each of the QuAC and TriviaQA trained models. On the other hand, T5 outperforms BART on the amalgam trained dataset for Bertscore-F1 and on the NQ trained dataset for USE.\n",
        "\n",
        "So, when we look just at the amalgam trained models, the results are indistinguishable across every metric other than bertscore-f1, where T5 outperforms BART."
      ],
      "metadata": {
        "id": "qiB1BdLHFV9M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = evaluation_df[default_hyperparameter_mask]\n",
        "df = df.groupby([\"trained_on\", \"base_model\"]).mean()\n",
        "df = df.reset_index()\n",
        "training_sets = df.trained_on.unique()\n",
        "df"
      ],
      "metadata": {
        "id": "Tzadu-f4Fsfc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reject = []\n",
        "fail_to_reject = []\n",
        "\n",
        "print(\"Reject the null hypothesis except where noted otherwise\\n\")\n",
        "\n",
        "for training_set in training_sets:\n",
        "    T5_set = df_T5[df_T5[\"trained_on\"] == training_set]\n",
        "    BART_set = df_BART[df_BART[\"trained_on\"] == training_set]\n",
        "\n",
        "    for metric in evaluation_metrics:\n",
        "        p = st.ttest_ind(T5_set[metric], BART_set[metric], equal_var=False)[1]\n",
        "        if p <= 0.05:\n",
        "            reject_null = \"\"\n",
        "            reject.append((training_set, metric))\n",
        "        else:\n",
        "            reject_null = \"***FAIL TO REJECT THE NULL\"\n",
        "            fail_to_reject.append((training_set, metric))\n",
        "\n",
        "        print(\n",
        "            f\"training set: {training_set} metric: {metric} p value: {p}, {reject_null}\"\n",
        "        )\n",
        "\n",
        "print(\n",
        "    f\"\\nreject the null: {len(reject)}, fail to reject the null:  {len(fail_to_reject)}\"\n",
        ")\n",
        "\n",
        "print(\"reject the null:\")\n",
        "pprint(reject)"
      ],
      "metadata": {
        "id": "2fusb5iRFsvk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bart_wins = 0\n",
        "t5_wins = 0\n",
        "\n",
        "print(\"BART wins the comparison except where noted otherwise\\n\")\n",
        "\n",
        "for training_set, metric in reject:\n",
        "    print(f\"training set: {training_set}\\tmetric: {metric}\", end=\" \")\n",
        "    T5 = df[(df[\"base_model\"] == \"T5\") & (df[\"trained_on\"] == training_set)][\n",
        "        metric\n",
        "    ].values[0]\n",
        "    BART = df[(df[\"base_model\"] == \"bart\") & (df[\"trained_on\"] == training_set)][\n",
        "        metric\n",
        "    ].values[0]\n",
        "\n",
        "    if BART > T5:\n",
        "        print(\"\")\n",
        "        bart_wins += 1\n",
        "    else:\n",
        "        print(\"\\t***T5 wins***\")\n",
        "        t5_wins += 1\n",
        "\n",
        "print()\n",
        "print(f\"BART wins = {bart_wins} T5 wins = {t5_wins}\")"
      ],
      "metadata": {
        "id": "Fd4RxeTJFspz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = evaluation_df[default_hyperparameter_mask]\n",
        "df = df[df[\"trained_on\"] == \"amalgam\"]"
      ],
      "metadata": {
        "id": "RGNJeL2hqYhK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"For Amalgam Trained Models\")\n",
        "fig, axes = plt.subplots(2, 3, figsize=(15, 6))\n",
        "for ax, metric in zip(axes.flat, evaluation_metrics):\n",
        "    sns.histplot(x=metric, data=df, hue=\"base_model\", ax=ax)\n",
        "list(axes.flat)[-1].remove()\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "x--TD2yrpweT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Which Model Is Best?\n",
        "\n",
        "Next we look at each of our models to see which performs the best when we consider every inference made with that model.  We consider each of our evaluation metrics.\n",
        "\n",
        "For each metric, the top two models are always the amalgram trained models and the next two models are always the squad trained models.\n",
        "\n",
        "As we discussed above, the only statistically significant difference between the amalgam trained models is that T5 outperforms BART on bertscore-F1 and there is not statistically significant difference between how the SQuAD trained models perform on any metric.\n",
        "\n",
        "So, our amalgam trained models are at the top with a slight edge to T5 over BART on those two models."
      ],
      "metadata": {
        "id": "6KYFpaI4y2-W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = evaluation_df[default_hyperparameter_mask]\n",
        "df = df.groupby([\"base_model\", \"trained_on\"]).mean()\n",
        "for metric in evaluation_metrics:\n",
        "    print(metric)\n",
        "    pprint(df[metric].sort_values(ascending=False)[:5])\n",
        "    print()"
      ],
      "metadata": {
        "id": "-kGoID61_u0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sequentially Trained Blended Models\n",
        "\n",
        "We compare the sn, snq and snqt models using the same methodology applied in the last section.  We look at every inference made and then each metric.\n",
        "\n",
        "For all metrics, the snqt model is the highest performer of the three sequentially trained models.  There is some variation between the second and third performers.  All differences are statistically significant.\n",
        "\n",
        "While snqt is the best of the sequentially trained models, it never outperforms the blended model that was trained on shuffled data."
      ],
      "metadata": {
        "id": "2aE97JPN3tVp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filename = evaluation_root + \"evaluation_database_snqt.json\"\n",
        "\n",
        "evaluation_df_2 = pd.read_json(filename)"
      ],
      "metadata": {
        "id": "r_s0rBtQ32GA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "default_hyperparameter_mask_2 = [\n",
        "    sample[\"defaults\"] for sample in evaluation_df_2.hyperparameter\n",
        "]\n",
        "evaluation_df_2 = evaluation_df_2.drop(columns=evaluation_exclusions)"
      ],
      "metadata": {
        "id": "zlrn3nLR41SX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### snqt outperforms sn and snq for each metric, all differences are statistically significant."
      ],
      "metadata": {
        "id": "YmsBuIoYPIAz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = evaluation_df_2[default_hyperparameter_mask_2]\n",
        "df = (\n",
        "    df.groupby([\"base_model\", \"trained_on\"])\n",
        "    .mean()\n",
        "    .sort_values(by=\"use\", ascending=False)\n",
        ")\n",
        "df"
      ],
      "metadata": {
        "id": "cy6lHYeO4kZr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for metric in evaluation_metrics:\n",
        "    print(metric)\n",
        "    pprint(df[metric].sort_values(ascending=False)[:3])\n",
        "    print()"
      ],
      "metadata": {
        "id": "BoynwyTP6d5d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = evaluation_df_2[default_hyperparameter_mask_2]\n",
        "\n",
        "snqt_set = df[df[\"trained_on\"] == \"snqt\"]\n",
        "\n",
        "reject = []\n",
        "fail_to_reject = []\n",
        "\n",
        "print(\"Reject the null hypothesis except where noted otherwise\\n\")\n",
        "\n",
        "for training_set in [\"sn\", \"snq\"]:\n",
        "    candidate_set = df[df[\"trained_on\"] == training_set]\n",
        "\n",
        "    for metric in evaluation_metrics:\n",
        "        p = st.ttest_ind(snqt_set[metric], candidate_set[metric], equal_var=False)[1]\n",
        "        if p <= 0.05:\n",
        "            reject_null = \"\"\n",
        "            reject.append((training_set, metric))\n",
        "        else:\n",
        "            reject_null = \"***FAIL TO REJECT THE NULL\"\n",
        "            fail_to_reject.append((training_set, metric))\n",
        "\n",
        "        print(\n",
        "            f\"training set: {training_set} metric: {metric} p value: {p}, {reject_null}\"\n",
        "        )\n",
        "\n",
        "print(\n",
        "    f\"\\nreject the null: {len(reject)}, fail to reject the null: {len(fail_to_reject)}\"\n",
        ")"
      ],
      "metadata": {
        "id": "Ebwf51FdTTP_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### BART amalgam outperforms BART snqt on every metric and in each case, the performance difference is statistically significant."
      ],
      "metadata": {
        "id": "rT89GEsOSl3p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_1 = df_BART[df_BART[\"trained_on\"] == \"amalgam\"]\n",
        "df_2 = evaluation_df_2[evaluation_df_2[\"trained_on\"] == \"snqt\"]\n",
        "df = pd.concat([df_1, df_2], axis=0, ignore_index=True)\n",
        "df = df.groupby([\"base_model\", \"trained_on\"]).mean()\n",
        "df"
      ],
      "metadata": {
        "id": "rXzNgG5t8Qcx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reject = []\n",
        "fail_to_reject = []\n",
        "\n",
        "print(\"Reject the null hypothesis except where noted otherwise\\n\")\n",
        "\n",
        "for metric in evaluation_metrics:\n",
        "    p = st.ttest_ind(df_1[metric], df_2[metric], equal_var=False)[1]\n",
        "    if p <= 0.05:\n",
        "        reject_null = \"\"\n",
        "        reject.append((metric))\n",
        "    else:\n",
        "        reject_null = \"***FAIL TO REJECT THE NULL\"\n",
        "        fail_to_reject.append((metric))\n",
        "\n",
        "    print(f\"metric: {metric} p value: {p}, {reject_null}\")\n",
        "\n",
        "print(\n",
        "    f\"\\nreject the null: {len(reject)}, fail to reject the null:  {len(fail_to_reject)}\"\n",
        ")"
      ],
      "metadata": {
        "id": "nlStVbqp-J93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "amalgam_wins = 0\n",
        "snqt_wins = 0\n",
        "df = df.reset_index()\n",
        "print(\"Amalgam wins the comparison except where noted otherwise\\n\")\n",
        "\n",
        "for metric in reject:\n",
        "    print(f\"metric: {metric}\", end=\" \")\n",
        "    amalgam = df[(df[\"trained_on\"] == \"amalgam\")][metric].values[0]\n",
        "    snqt = df[(df[\"trained_on\"] == \"snqt\")][metric].values[0]\n",
        "\n",
        "    if amalgam > snqt:\n",
        "        print(\"\")\n",
        "        amalgam_wins += 1\n",
        "    else:\n",
        "        print(\"\\t***snqt wins***\")\n",
        "        snqt_wins += 1\n",
        "\n",
        "print()\n",
        "print(f\"amalgam wins = {amalgam_wins} snqt wins = {snqt_wins}\")"
      ],
      "metadata": {
        "id": "x__4O15QAb2Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.concat([df_1, df_2], axis=0, ignore_index=True)\n",
        "print(\"Compare BART Amalgam v BART snqt (all inferences)\")\n",
        "fig, axes = plt.subplots(2, 3, figsize=(15, 6))\n",
        "for ax, metric in zip(axes.flat, evaluation_metrics):\n",
        "    sns.histplot(x=metric, data=df, hue=\"trained_on\", ax=ax)\n",
        "list(axes.flat)[-1].remove()\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "_S0lpv2erpn1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BART SQuAD compared to BART snqt\n",
        "\n",
        "Does BART snqt manage to outperform BART SQuAD even though it does not outperform BART amalgam?\n",
        "\n",
        "Not really.  BART SQuAD outperforms BART snqt across the board with the exception of meteor.\n",
        "\n",
        "All differences are statistically significant."
      ],
      "metadata": {
        "id": "6ODUNglCL1uc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_3 = df_BART[df_BART[\"trained_on\"] == \"squad\"]\n",
        "df = pd.concat([df_2, df_3], axis=0, ignore_index=True)\n",
        "df = (\n",
        "    df.groupby([\"base_model\", \"trained_on\"])\n",
        "    .mean()\n",
        "    .sort_values(by=\"use\", ascending=False)\n",
        ")\n",
        "df"
      ],
      "metadata": {
        "id": "4ZYKIO5QL_mx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reject = []\n",
        "fail_to_reject = []\n",
        "\n",
        "print(\"Reject the null hypothesis except where noted otherwise\\n\")\n",
        "\n",
        "for metric in evaluation_metrics:\n",
        "    p = st.ttest_ind(df_2[metric], df_3[metric], equal_var=False)[1]\n",
        "    if p <= 0.05:\n",
        "        reject_null = \"\"\n",
        "        reject.append((metric))\n",
        "    else:\n",
        "        reject_null = \"***FAIL TO REJECT THE NULL\"\n",
        "        fail_to_reject.append((metric))\n",
        "\n",
        "    print(f\"metric: {metric} p value: {p}, {reject_null}\")\n",
        "\n",
        "print(\n",
        "    f\"\\nreject the null: {len(reject)}, fail to reject the null:  {len(fail_to_reject)}\"\n",
        ")"
      ],
      "metadata": {
        "id": "UwhRxqbTMaQj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "snqt_wins = 0\n",
        "squad_wins = 0\n",
        "df = df.reset_index()\n",
        "\n",
        "for metric in reject:\n",
        "    print(f\"metric: {metric}\", end=\" \")\n",
        "    snqt = df[(df[\"trained_on\"] == \"snqt\")][metric].values[0]\n",
        "    squad = df[(df[\"trained_on\"] == \"squad\")][metric].values[0]\n",
        "\n",
        "    if squad > snqt:\n",
        "        print(\"\\tSQuAD trained wins\")\n",
        "        squad_wins += 1\n",
        "    else:\n",
        "        print(\"\\tsnqt trained wins\")\n",
        "        snqt_wins += 1\n",
        "\n",
        "print()\n",
        "print(f\"SQuAD wins = {squad_wins} snqt wins = {snqt_wins}\")"
      ],
      "metadata": {
        "id": "2zxvpf-OM7MY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.concat([df_2, df_3], axis=0, ignore_index=True)\n",
        "print(\"Compare BART snqt v BART SQuAD (all inferences)\")\n",
        "fig, axes = plt.subplots(2, 3, figsize=(15, 6))\n",
        "for ax, metric in zip(axes.flat, evaluation_metrics):\n",
        "    sns.histplot(x=metric, data=df, hue=\"trained_on\", ax=ax)\n",
        "list(axes.flat)[-1].remove()\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "fyn3PW2pK3IL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Beam search and no repeat ngrams on TriviaQA\n",
        "\n"
      ],
      "metadata": {
        "id": "s6XRvkI33MMR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We ran the full suite of inference tests on our BART models with beam search and the use of a no repeat ngram parameters.  The primnary motivation for this experiment was to see if we would get better performance on the TriviaQA validation set.  As shown below, our top performance on each metric we consider remains the model trained on the amalgamated dataset and running inference without beam search or the use of a no repeat ngram parameter."
      ],
      "metadata": {
        "id": "0OpRfiEjCxbj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for metric in evaluation_metrics:\n",
        "    df = df_BART[[\"nickname\", \"trained_on\", \"tested_on\", metric]]\n",
        "    df = df[df.tested_on == \"triviaqa\"]\n",
        "    display(\n",
        "        df.groupby([\"nickname\", \"trained_on\", \"tested_on\"])\n",
        "        .mean()\n",
        "        .sort_values([metric], ascending=False)[:4]\n",
        "    )"
      ],
      "metadata": {
        "id": "8xjBrf_s3XRz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Kendall's Tau"
      ],
      "metadata": {
        "id": "7vP_qHhY09ZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = evaluation_df[default_hyperparameter_mask]\n",
        "baseline = (\n",
        "    df[\n",
        "        (df[\"base_model\"] == \"T5\")\n",
        "        & (df[\"trained_on\"] == \"squad\")\n",
        "        & (df[\"tested_on\"] == \"squad\")\n",
        "    ]\n",
        "    .groupby([\"base_model\", \"trained_on\", \"tested_on\"])\n",
        "    .mean()\n",
        ")\n",
        "baseline"
      ],
      "metadata": {
        "id": "2p-1A4cjbCQg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nq_df = (\n",
        "    df[df[\"tested_on\"] == \"nq\"]\n",
        "    .groupby([\"base_model\", \"trained_on\", \"tested_on\"])\n",
        "    .mean()\n",
        "    .sort_values(by=[\"bertscore-f1\"], ascending=False)\n",
        ")\n",
        "\n",
        "nq_df"
      ],
      "metadata": {
        "id": "qiyNCJITTt7i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quac_df = (\n",
        "    df[df[\"tested_on\"] == \"quac\"]\n",
        "    .groupby([\"base_model\", \"trained_on\", \"tested_on\"])\n",
        "    .mean()\n",
        "    .sort_values(by=[\"bertscore-f1\"], ascending=False)\n",
        ")\n",
        "\n",
        "quac_df"
      ],
      "metadata": {
        "id": "oFfILqTjV4FD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "squad_df = (\n",
        "    df[df[\"tested_on\"] == \"squad\"]\n",
        "    .groupby([\"base_model\", \"trained_on\", \"tested_on\"])\n",
        "    .mean()\n",
        "    .sort_values(by=[\"bertscore-f1\"], ascending=False)\n",
        ")\n",
        "\n",
        "squad_df"
      ],
      "metadata": {
        "id": "pJW3oDRMYjR-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "triviaqa_df = (\n",
        "    df[df[\"tested_on\"] == \"triviaqa\"]\n",
        "    .groupby([\"base_model\", \"trained_on\", \"tested_on\"])\n",
        "    .mean()\n",
        "    .sort_values(by=[\"bertscore-f1\"], ascending=False)\n",
        ")\n",
        "\n",
        "triviaqa_df"
      ],
      "metadata": {
        "id": "UVFuDKwXYtSt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = list(squad_df.columns)\n",
        "datasets = df[\"tested_on\"].unique()\n",
        "print(f\"metrics = {metrics} \\ndatasets = {datasets}\")"
      ],
      "metadata": {
        "id": "RGLspwsRp4fG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We gather the predictions for a validation dataset\n",
        "# Sort the rows from best to worst scores on one metric\n",
        "# Then we build ranking vectors for all metrics\n",
        "\n",
        "rankings = {}\n",
        "\n",
        "for dataset in datasets:\n",
        "    slice_df = (\n",
        "        df[df[\"tested_on\"] == dataset]\n",
        "        .groupby([\"base_model\", \"trained_on\", \"tested_on\"])\n",
        "        .mean()\n",
        "        .sort_values(by=[\"bleu\"], ascending=False)\n",
        "    )\n",
        "    for metric in metrics:\n",
        "        vector = [\n",
        "            sorted(slice_df[metric], reverse=True).index(x) + 1\n",
        "            for x in slice_df[metric]\n",
        "        ]\n",
        "        rankings[(dataset, metric)] = vector"
      ],
      "metadata": {
        "id": "1TjrEIVjtpfv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rankings"
      ],
      "metadata": {
        "id": "vEcTzJhx0d99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectors = {}\n",
        "for dataset in datasets:\n",
        "    vectors[dataset] = {metric: rankings[(dataset, metric)] for metric in metrics}\n",
        "vectors"
      ],
      "metadata": {
        "id": "8V6xewDMnyWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ranking_df = {dataset: pd.DataFrame(vectors[dataset]) for dataset in datasets}\n",
        "for k, v in ranking_df.items():\n",
        "    print(k)\n",
        "    print(v)"
      ],
      "metadata": {
        "id": "2-UZdX3NopCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kendall_df = {\n",
        "    dataset: ranking_df[dataset].corr(method=\"kendall\") for dataset in datasets\n",
        "}\n",
        "for k, v in kendall_df.items():\n",
        "    print(k)\n",
        "    print(v)"
      ],
      "metadata": {
        "id": "YcC7EKjSqP-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# It does not matter which set we use to build the mask\n",
        "mask = np.triu(np.ones_like(kendall_df[\"squad\"].corr(), dtype=bool))\n",
        "mask"
      ],
      "metadata": {
        "id": "VE_IAmOXrmrJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(2, 2, figsize=(9, 9))\n",
        "\n",
        "for ax, dataset in zip(axes.flat, datasets):\n",
        "    sns.heatmap(kendall_df[dataset], mask=mask, cmap=\"Blues\", ax=ax).set(title=dataset)\n",
        "\n",
        "fig.tight_layout()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DdNh6cCKrx3Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cosine Similarity"
      ],
      "metadata": {
        "id": "1OfGhg_snQN5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectors = {}\n",
        "for dataset in datasets:\n",
        "    vectors[dataset] = [rankings[(dataset, metric)] for metric in metrics]"
      ],
      "metadata": {
        "id": "z7S8uYCw1Ge8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "similarity = {}\n",
        "for dataset in datasets:\n",
        "    similarity[dataset] = pd.DataFrame(\n",
        "        cosine_similarity(vectors[dataset]), columns=metrics, index=metrics\n",
        "    )"
      ],
      "metadata": {
        "id": "RnYRs0YZ6A3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "similarity[\"nq\"]"
      ],
      "metadata": {
        "id": "Os99T1Fx7eO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "similarity[\"quac\"]"
      ],
      "metadata": {
        "id": "wrzCS3d07elD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "similarity[\"squad\"]"
      ],
      "metadata": {
        "id": "s72iKPpT7ens"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "similarity[\"triviaqa\"]"
      ],
      "metadata": {
        "id": "bAuPlrz37eqf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# It does not matter which set we use to build the mask\n",
        "mask = np.triu(np.ones_like(similarity[\"squad\"].corr(), dtype=bool))"
      ],
      "metadata": {
        "id": "VeKOKB197GZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(2, 2, figsize=(9, 9))\n",
        "\n",
        "for ax, dataset in zip(axes.flat, datasets):\n",
        "    sns.heatmap(similarity[dataset], mask=mask, cmap=\"coolwarm\", ax=ax).set(\n",
        "        title=dataset\n",
        "    )\n",
        "\n",
        "fig.tight_layout()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fdAj_1Au1Gp7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Black"
      ],
      "metadata": {
        "id": "Mcd_R5JUGCxV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install black[jupyter]"
      ],
      "metadata": {
        "id": "N9mKRIlQnygg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!black \"/content/drive/MyDrive/Colab Notebooks/rr_evaluation_sandbox.ipynb\""
      ],
      "metadata": {
        "id": "dOT0YkhFokyI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}