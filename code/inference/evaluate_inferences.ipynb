{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "5UkrEtmAtamY",
        "7Dr3uT0ht8_j"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOfpx/+2cZXZeXKU2dVM3Hb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeanlucjackson/w266_final_project/blob/main/code/inference/evaluate_inferences.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports & Google Drive Mounting"
      ],
      "metadata": {
        "id": "0ti7N7kgi9k-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "\n",
        "import csv\n",
        "import pprint\n",
        "\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "sEho-NjGi9VU"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HjsB3ST7hdDE",
        "outputId": "2baeee23-6465-408b-bc78-ab8ba31c95dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# This cell will authenticate you and mount your Drive in the Colab.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inference_root = \"/content/drive/MyDrive/w266 NLP Final Project/Predictions/\""
      ],
      "metadata": {
        "id": "2Uu8rbzeisNt"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inference_files = listdir(inference_root)\n",
        "pprint.pprint(inference_files)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ly7uAmyOi7mc",
        "outputId": "040efc51-3e3c-4c60-815d-8d6841340515"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['t5_simple_transformers_preds.csv',\n",
            " 'predictions.T5_base_pt.squad.quac.csv',\n",
            " 'predictions.T5_base_pt.squad.squad.csv',\n",
            " 'predictions.T5_base_pt.quac.squad.csv',\n",
            " 'predictions.T5_base_pt.quac.quac.csv',\n",
            " 'predictions.T5_base_pt.squad.nq.csv',\n",
            " 'predictions.T5_base_pt.quac.nq.csv',\n",
            " 'predictions.T5_base_pt_long.squad.triviaqa.csv',\n",
            " 'predictions.T5_base_pt_long.quac.triviaqa.csv',\n",
            " 'predictions.bart_base_pt.squad.squad.csv']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluations\n",
        "## Load Data\n",
        "\n",
        "Inferences will be saved into the `inference_dict` nested dictionary, whose format is:\n",
        "- keys: CSV filenames\n",
        "- values:\n",
        "  - `target`: list of target values\n",
        "  - `prediction`: list of prediction values"
      ],
      "metadata": {
        "id": "PBHNV6rdjcjE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inference_dict = {}\n",
        "\n",
        "for id, inf_file in enumerate(inference_files):\n",
        "\n",
        "  # Load CSV file containing predictions\n",
        "  filename = join(inference_root, inf_file)\n",
        "  \n",
        "  # If the file exists, load it into pandas\n",
        "  if isfile(filename):\n",
        "    print(f\"Opening file {id + 1} of {len(inference_files)}: {inf_file}\\n\")\n",
        "\n",
        "    df = pd.read_csv(filename)\n",
        "    \n",
        "    # If the CSV does not have correct column names, warn user and skip file\n",
        "    if 'target' not in df.columns and 'prediction' not in df.columns:\n",
        "      print(\"WARNING: Columns `target` and `prediction` not found in CSV. Skipping CSV.\")\n",
        "      print(f\"Check file: {filename}\")\n",
        "      # continue\n",
        "\n",
        "    # Columns exist, so continue\n",
        "    else:\n",
        "      targets = df['target']\n",
        "      predictions = df['prediction']\n",
        "\n",
        "      print('CSV loaded.')\n",
        "      print(f\"Length of targets:      {len(targets)}\")\n",
        "      print(f\"Length of predictions:  {len(predictions)}\")\n",
        "      \n",
        "      # Save lists into prediction dictionary under file's name\n",
        "      inference_dict.update(\n",
        "          {inf_file: {'target': targets,\n",
        "                      'prediction': predictions}\n",
        "          }\n",
        "      )\n",
        "      print('\\nTargets and predictions saved.')\n",
        "    \n",
        "    print('________________________________________\\n')\n",
        "\n",
        "\n",
        "print(f\"\\nTotal of {len(inference_dict.keys())} datasets loaded:\")\n",
        "for dataset in inference_dict.keys():\n",
        "  print('    ' + dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kif-BL0OoxA2",
        "outputId": "73b60cbd-a596-464d-adad-c6f886ebccc4"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Opening file 1 of 10: t5_simple_transformers_preds.csv\n",
            "\n",
            "WARNING: Columns `target` and `prediction` not found in CSV. Skipping CSV.\n",
            "Check file: /content/drive/MyDrive/w266 NLP Final Project/Predictions/t5_simple_transformers_preds.csv\n",
            "________________________________________\n",
            "\n",
            "Opening file 2 of 10: predictions.T5_base_pt.squad.quac.csv\n",
            "\n",
            "CSV loaded.\n",
            "Length of targets:      5868\n",
            "Length of predictions:  5868\n",
            "\n",
            "Targets and predictions saved.\n",
            "________________________________________\n",
            "\n",
            "Opening file 3 of 10: predictions.T5_base_pt.squad.squad.csv\n",
            "\n",
            "CSV loaded.\n",
            "Length of targets:      10570\n",
            "Length of predictions:  10570\n",
            "\n",
            "Targets and predictions saved.\n",
            "________________________________________\n",
            "\n",
            "Opening file 4 of 10: predictions.T5_base_pt.quac.squad.csv\n",
            "\n",
            "CSV loaded.\n",
            "Length of targets:      10570\n",
            "Length of predictions:  10570\n",
            "\n",
            "Targets and predictions saved.\n",
            "________________________________________\n",
            "\n",
            "Opening file 5 of 10: predictions.T5_base_pt.quac.quac.csv\n",
            "\n",
            "CSV loaded.\n",
            "Length of targets:      5868\n",
            "Length of predictions:  5868\n",
            "\n",
            "Targets and predictions saved.\n",
            "________________________________________\n",
            "\n",
            "Opening file 6 of 10: predictions.T5_base_pt.squad.nq.csv\n",
            "\n",
            "CSV loaded.\n",
            "Length of targets:      2356\n",
            "Length of predictions:  2356\n",
            "\n",
            "Targets and predictions saved.\n",
            "________________________________________\n",
            "\n",
            "Opening file 7 of 10: predictions.T5_base_pt.quac.nq.csv\n",
            "\n",
            "CSV loaded.\n",
            "Length of targets:      2356\n",
            "Length of predictions:  2356\n",
            "\n",
            "Targets and predictions saved.\n",
            "________________________________________\n",
            "\n",
            "Opening file 8 of 10: predictions.T5_base_pt_long.squad.triviaqa.csv\n",
            "\n",
            "CSV loaded.\n",
            "Length of targets:      9835\n",
            "Length of predictions:  9835\n",
            "\n",
            "Targets and predictions saved.\n",
            "________________________________________\n",
            "\n",
            "Opening file 9 of 10: predictions.T5_base_pt_long.quac.triviaqa.csv\n",
            "\n",
            "CSV loaded.\n",
            "Length of targets:      9836\n",
            "Length of predictions:  9836\n",
            "\n",
            "Targets and predictions saved.\n",
            "________________________________________\n",
            "\n",
            "Opening file 10 of 10: predictions.bart_base_pt.squad.squad.csv\n",
            "\n",
            "CSV loaded.\n",
            "Length of targets:      10570\n",
            "Length of predictions:  10570\n",
            "\n",
            "Targets and predictions saved.\n",
            "________________________________________\n",
            "\n",
            "\n",
            "Total of 9 datasets loaded:\n",
            "    predictions.T5_base_pt.squad.quac.csv\n",
            "    predictions.T5_base_pt.squad.squad.csv\n",
            "    predictions.T5_base_pt.quac.squad.csv\n",
            "    predictions.T5_base_pt.quac.quac.csv\n",
            "    predictions.T5_base_pt.squad.nq.csv\n",
            "    predictions.T5_base_pt.quac.nq.csv\n",
            "    predictions.T5_base_pt_long.squad.triviaqa.csv\n",
            "    predictions.T5_base_pt_long.quac.triviaqa.csv\n",
            "    predictions.bart_base_pt.squad.squad.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll be using:\n",
        "- ROUGE\n",
        "- BLEU-RT\n",
        "- BERTScore\n",
        "- METEOR\n",
        "- USE\n",
        "\n",
        "And storing evaluations in `evaluation_dict` formatted as:\n",
        "- keys: CSV filenames\n",
        "- values:\n",
        "  - metric_name: metric_value"
      ],
      "metadata": {
        "id": "FUw1vCoos3Na"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Evaluation Metrics"
      ],
      "metadata": {
        "id": "svociRbhtP7s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q evaluate\n",
        "import evaluate"
      ],
      "metadata": {
        "id": "e3tiJPqOsyll"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ROUGE\n",
        "🤗 [ROUGE page](https://huggingface.co/spaces/evaluate-metric/rouge)"
      ],
      "metadata": {
        "id": "5UkrEtmAtamY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q rouge_score\n",
        "\n",
        "rouge = evaluate.load('rouge')"
      ],
      "metadata": {
        "id": "0Nc9eaqLtXt4"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BLEU-RT\n",
        "- No fine-tuning yet.\n",
        "- Using `BLEURT-20` checkpoint per Google's recommendation (see [BLEURT GitHub page](https://github.com/google-research/bleurt/blob/master/checkpoints.md#the-recommended-checkpoint-bleurt-20))"
      ],
      "metadata": {
        "id": "7Dr3uT0ht8_j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/google-research/bleurt.git\n",
        "\n",
        "bleurt = evaluate.load('bleurt', module_type='metric', checkpoint='BLEURT-20-D3')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSStQ_cntd7W",
        "outputId": "4f96757a-d44f-49f0-cd61-4fd6a2ac3f0d"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/google-research/bleurt.git\n",
            "  Cloning https://github.com/google-research/bleurt.git to /tmp/pip-req-build-jz5bxvkv\n",
            "  Running command git clone -q https://github.com/google-research/bleurt.git /tmp/pip-req-build-jz5bxvkv\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from BLEURT==0.0.2) (1.3.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from BLEURT==0.0.2) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from BLEURT==0.0.2) (1.7.3)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (from BLEURT==0.0.2) (2.9.2)\n",
            "Requirement already satisfied: tf-slim>=1.1 in /usr/local/lib/python3.7/dist-packages (from BLEURT==0.0.2) (1.1.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from BLEURT==0.0.2) (0.1.97)\n",
            "Requirement already satisfied: absl-py>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from tf-slim>=1.1->BLEURT==0.0.2) (1.3.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->BLEURT==0.0.2) (2022.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->BLEURT==0.0.2) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->BLEURT==0.0.2) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow->BLEURT==0.0.2) (4.1.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow->BLEURT==0.0.2) (57.4.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->BLEURT==0.0.2) (0.27.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->BLEURT==0.0.2) (3.3.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->BLEURT==0.0.2) (2.9.0)\n",
            "Requirement already satisfied: flatbuffers<2,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow->BLEURT==0.0.2) (1.12)\n",
            "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->BLEURT==0.0.2) (2.9.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->BLEURT==0.0.2) (0.4.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->BLEURT==0.0.2) (1.14.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->BLEURT==0.0.2) (2.0.1)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->BLEURT==0.0.2) (3.1.0)\n",
            "Requirement already satisfied: tensorboard<2.10,>=2.9 in /usr/local/lib/python3.7/dist-packages (from tensorflow->BLEURT==0.0.2) (2.9.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow->BLEURT==0.0.2) (1.50.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->BLEURT==0.0.2) (1.1.2)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->BLEURT==0.0.2) (3.17.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->BLEURT==0.0.2) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->BLEURT==0.0.2) (14.0.6)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from tensorflow->BLEURT==0.0.2) (21.3)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->BLEURT==0.0.2) (1.6.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow->BLEURT==0.0.2) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow->BLEURT==0.0.2) (1.5.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow->BLEURT==0.0.2) (3.4.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow->BLEURT==0.0.2) (0.4.6)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow->BLEURT==0.0.2) (1.0.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow->BLEURT==0.0.2) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow->BLEURT==0.0.2) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow->BLEURT==0.0.2) (1.8.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow->BLEURT==0.0.2) (2.23.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow->BLEURT==0.0.2) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow->BLEURT==0.0.2) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow->BLEURT==0.0.2) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow->BLEURT==0.0.2) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow->BLEURT==0.0.2) (4.13.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow->BLEURT==0.0.2) (3.10.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow->BLEURT==0.0.2) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow->BLEURT==0.0.2) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow->BLEURT==0.0.2) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow->BLEURT==0.0.2) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow->BLEURT==0.0.2) (1.25.11)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow->BLEURT==0.0.2) (3.2.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->tensorflow->BLEURT==0.0.2) (3.0.9)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:evaluate_modules.metrics.evaluate-metric--bleurt.98e148b2f8c4a88aba5037e4e0e90c9fd9ec35dc37a054ded8cfef0fa801ffab.bleurt:Using default BLEURT-Base checkpoint for sequence maximum length 128. You can use a bigger model for better results with e.g.: evaluate.load('bleurt', 'bleurt-large-512').\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BERTScore\n",
        "🤗 [BERTScore page](https://huggingface.co/spaces/evaluate-metric/bertscore)\n",
        "- Using `distilbert-base-uncased` per 🤗 recommendation because the default model (`roberta-large`) is over 1.4GB"
      ],
      "metadata": {
        "id": "XziuT1wluJJn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bert_score\n",
        "\n",
        "bertscore = evaluate.load('bertscore')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ovqwrlx7ti97",
        "outputId": "6149748f-0db2-41c6-949c-9874c1768b68"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: bert_score in /usr/local/lib/python3.7/dist-packages (0.3.12)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from bert_score) (3.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from bert_score) (1.21.6)\n",
            "Requirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.7/dist-packages (from bert_score) (4.64.1)\n",
            "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from bert_score) (1.3.5)\n",
            "Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from bert_score) (4.24.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.7/dist-packages (from bert_score) (21.3)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from bert_score) (1.12.1+cu113)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from bert_score) (2.23.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.9->bert_score) (3.0.9)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0.1->bert_score) (2022.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0.1->bert_score) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=1.0.1->bert_score) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.0.0->bert_score) (4.1.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.0->bert_score) (0.13.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.0->bert_score) (6.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.0->bert_score) (0.10.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.0->bert_score) (4.13.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.0->bert_score) (2022.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.0->bert_score) (3.8.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers>=3.0.0->bert_score) (3.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->bert_score) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->bert_score) (0.11.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->bert_score) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->bert_score) (1.25.11)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->bert_score) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->bert_score) (2.10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### METEOR\n",
        "🤗 [METEOR page](https://huggingface.co/spaces/evaluate-metric/meteor)"
      ],
      "metadata": {
        "id": "TuQR5UsVByhb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "meteor = evaluate.load('meteor')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RECwk3-JrJOM",
        "outputId": "f48083a4-a9a1-48c6-fd7a-a08972991ee1"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Universal Sentence Encoder (USE) – PENDING"
      ],
      "metadata": {
        "id": "li2nrtfWO8k0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculate Metrics on Each Dataset\n",
        "\n",
        "Metrics are calculated for each `target` – `prediction` pair. These pairs are averaged for each dataset to have a single value to compare between models and datasets."
      ],
      "metadata": {
        "id": "tOwFwl_7uXse"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluations will be stored in dictionary\n",
        "\n",
        "evaluation_dict = {}"
      ],
      "metadata": {
        "id": "otfo6eTfPsVQ"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ROUGE"
      ],
      "metadata": {
        "id": "8ZPuxr9LOcp-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for id, dataset in enumerate(inference_dict.keys()):\n",
        "  print(f\"Evaluating ROUGE on {dataset}...\")\n",
        "\n",
        "  targets = inference_dict[dataset]['target'].tolist()\n",
        "  predictions = inference_dict[dataset]['prediction'].tolist()\n",
        "\n",
        "  # ROUGE scores\n",
        "  # The use_aggregator argument takes the average for us\n",
        "  rouge_results = rouge.compute(predictions=predictions,\n",
        "                                references=targets,\n",
        "                                use_aggregator=True)\n",
        "  \n",
        "  for metric in rouge_results:\n",
        "    \n",
        "    # If this dataset hasn't been added to dict, add it and metric\n",
        "    if not evaluation_dict.get(dataset):\n",
        "      evaluation_dict.update(\n",
        "            {\n",
        "                dataset: {metric: rouge_results[metric]}\n",
        "            }\n",
        "        )\n",
        "      \n",
        "    # This dataset already exists as a key, so add this metric\n",
        "    else:\n",
        "      evaluation_dict[dataset].update(\n",
        "          {\n",
        "              metric: rouge_results[metric]\n",
        "          }\n",
        "      )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ftr9os6pRTou",
        "outputId": "eea616e9-eb34-454f-fc3d-d3283bff2bcd"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating ROUGE on predictions.T5_base_pt.squad.quac.csv...\n",
            "Evaluating ROUGE on predictions.T5_base_pt.squad.squad.csv...\n",
            "Evaluating ROUGE on predictions.T5_base_pt.quac.squad.csv...\n",
            "Evaluating ROUGE on predictions.T5_base_pt.quac.quac.csv...\n",
            "Evaluating ROUGE on predictions.T5_base_pt.squad.nq.csv...\n",
            "Evaluating ROUGE on predictions.T5_base_pt.quac.nq.csv...\n",
            "Evaluating ROUGE on predictions.T5_base_pt_long.squad.triviaqa.csv...\n",
            "Evaluating ROUGE on predictions.T5_base_pt_long.quac.triviaqa.csv...\n",
            "Evaluating ROUGE on predictions.bart_base_pt.squad.squad.csv...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test out on one of the datasets\n",
        "dataset = list(inference_dict.keys())[0]\n",
        "print(dataset)\n",
        "\n",
        "targets = inference_dict[dataset]['target'].tolist()\n",
        "predictions = inference_dict[dataset]['prediction'].tolist()\n",
        "print(len(targets), len(predictions))\n",
        "\n",
        "print('ROUGE')\n",
        "rouge_results = rouge.compute(predictions=predictions,\n",
        "                              references=targets,\n",
        "                              use_aggregator=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kdjCExuFyWOq",
        "outputId": "a28eff39-b00f-4d32-f78a-9f51124df814"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "predictions.T5_base_pt.squad.quac.csv\n",
            "5868 5868\n",
            "ROUGE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(rouge_results))\n",
        "print(len(rouge_results))\n",
        "print(rouge_results.keys())\n",
        "print(len(rouge_results['rouge1']))\n",
        "\n",
        "print(f\"Averages:\")\n",
        "for k in rouge_results.keys():\n",
        "  print(k + ': ', end='')\n",
        "  print(sum(rouge_results[k])/len(rouge_results[k]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XtDTAtCx1hyY",
        "outputId": "ec156668-51ed-4133-f3e3-fb581b8061b5"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'dict'>\n",
            "4\n",
            "dict_keys(['rouge1', 'rouge2', 'rougeL', 'rougeLsum'])\n",
            "5868\n",
            "Averages:\n",
            "rouge1: 0.18249635750457507\n",
            "rouge2: 0.03966958090769743\n",
            "rougeL: 0.17632388612567854\n",
            "rougeLsum: 0.17632388612567854\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for d in evaluation_dict.keys():\n",
        "  print(d)\n",
        "  pprint.pprint(evaluation_dict[d])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8HGoi70--enG",
        "outputId": "ef395281-ccec-4144-fe10-a6d423542411"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "predictions.T5_base_pt.squad.quac.csv\n",
            "{'meteor': 0.15452861614981653,\n",
            " 'rouge1': 0.18246685757067002,\n",
            " 'rouge2': 0.039596756029108796,\n",
            " 'rougeL': 0.17629216493034694,\n",
            " 'rougeLsum': 0.1763863380861867}\n",
            "predictions.T5_base_pt.squad.squad.csv\n",
            "{'meteor': 0.4430975371865821,\n",
            " 'rouge1': 0.4505391054758221,\n",
            " 'rouge2': 0.23630356484624362,\n",
            " 'rougeL': 0.41706970371613455,\n",
            " 'rougeLsum': 0.4169109103737757}\n",
            "predictions.T5_base_pt.quac.squad.csv\n",
            "{'meteor': 0.32373301147608025,\n",
            " 'rouge1': 0.2995440075780461,\n",
            " 'rouge2': 0.10090232093578526,\n",
            " 'rougeL': 0.27689684166147077,\n",
            " 'rougeLsum': 0.27709570427605157}\n",
            "predictions.T5_base_pt.quac.quac.csv\n",
            "{'meteor': 0.2247521536626319,\n",
            " 'rouge1': 0.21764427384319768,\n",
            " 'rouge2': 0.0718138922285132,\n",
            " 'rougeL': 0.21306161056965345,\n",
            " 'rougeLsum': 0.21318144470237674}\n",
            "predictions.T5_base_pt.squad.nq.csv\n",
            "{'meteor': 0.2694306204049121,\n",
            " 'rouge1': 0.34958469648362983,\n",
            " 'rouge2': 0.1639298816200002,\n",
            " 'rougeL': 0.3276778292684154,\n",
            " 'rougeLsum': 0.32779565382724274}\n",
            "predictions.T5_base_pt.quac.nq.csv\n",
            "{'meteor': 0.20906696581923326,\n",
            " 'rouge1': 0.25728935276041587,\n",
            " 'rouge2': 0.10031120086014222,\n",
            " 'rougeL': 0.2419926589441546,\n",
            " 'rougeLsum': 0.24207844859955746}\n",
            "predictions.T5_base_pt_long.squad.triviaqa.csv\n",
            "{'meteor': 0.31080680839927166,\n",
            " 'rouge1': 0.31188967683275043,\n",
            " 'rouge2': 0.11144286663987,\n",
            " 'rougeL': 0.2693780310845009,\n",
            " 'rougeLsum': 0.26926004183579144}\n",
            "predictions.T5_base_pt_long.quac.triviaqa.csv\n",
            "{'meteor': 0.15228926747532834,\n",
            " 'rouge1': 0.11572891189287132,\n",
            " 'rouge2': 0.011757222291305809,\n",
            " 'rougeL': 0.10806013642756193,\n",
            " 'rougeLsum': 0.10801181882867728}\n",
            "predictions.bart_base_pt.squad.squad.csv\n",
            "{'meteor': 0.49918084899063064,\n",
            " 'rouge1': 0.49870305386996494,\n",
            " 'rouge2': 0.2852314571949791,\n",
            " 'rougeL': 0.4653492280175048,\n",
            " 'rougeLsum': 0.4654403396891432}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BLEU-RT"
      ],
      "metadata": {
        "id": "Bzga2-JBq8O-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for id, dataset in enumerate(inference_dict.keys()):\n",
        "  print(f\"Evaluating BLEU-RT on {dataset}...\")\n",
        "\n",
        "  targets = inference_dict[dataset]['target'].tolist()\n",
        "  predictions = inference_dict[dataset]['prediction'].tolist()\n",
        "\n",
        "  # BLEU-RT scores\n",
        "  bleurt_results = bleurt.compute(predictions=predictions,\n",
        "                                  references=targets)\n",
        "  \n",
        "  # Average over scores\n",
        "  bleurt_scores_list = bleurt_results['scores']\n",
        "  avg_bleurt = sum(bleurt_scores_list) / len(bleurt_scores_list)\n",
        "    \n",
        "  # If this dataset hasn't been added to dict, add it and metric\n",
        "  if not evaluation_dict.get(dataset):\n",
        "    evaluation_dict.update(\n",
        "          {\n",
        "              dataset: {'bleurt': avg_bleurt}\n",
        "          }\n",
        "      )\n",
        "    \n",
        "  # This dataset already exists as a key, so add this metric\n",
        "  else:\n",
        "    evaluation_dict[dataset].update(\n",
        "        {\n",
        "            'bleurt': avg_bleurt\n",
        "        }\n",
        "    )\n"
      ],
      "metadata": {
        "id": "RTbgwIjkxH3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pprint.pprint(evaluation_dict)"
      ],
      "metadata": {
        "id": "b84WuMymyUKW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BERTScore"
      ],
      "metadata": {
        "id": "eUUWH7nTrAzS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for id, dataset in enumerate(inference_dict.keys()):\n",
        "  print(f\"Evaluating BERTScore on {dataset}...\")\n",
        "\n",
        "  targets = inference_dict[dataset]['target'].tolist()\n",
        "  predictions = inference_dict[dataset]['prediction'].tolist()\n",
        "\n",
        "  # BERT Scores\n",
        "  bertscore_results = bertscore.compute(predictions=predictions,\n",
        "                                        references=targets,\n",
        "                                        model_type='distilbert-base-uncased')\n",
        "  \n",
        "  # Average over scores\n",
        "  bertscore_precision_list = bertscore_results['precision']\n",
        "  bertscore_recall_list = bertscore_results['recall']\n",
        "  bertscore_f1_list = bertscore_results['f1']\n",
        "\n",
        "  avg_precision = sum(bertscore_precision_list) / len(bertscore_precision_list)\n",
        "  avg_recall = sum(bertscore_recall_list) / len(bertscore_recall_list)\n",
        "  avg_f1 = sum(bertscore_f1_list) / len(bertscore_f1_list)\n",
        "    \n",
        "  # If this dataset hasn't been added to dict, add it and metric\n",
        "  if not evaluation_dict.get(dataset):\n",
        "    evaluation_dict.update(\n",
        "          {\n",
        "              dataset: {'bertscore-precision': avg_precision,\n",
        "                        'bertscore-recall': avg_recall,\n",
        "                        'bertscore-f1': avg_f1}\n",
        "          }\n",
        "      )\n",
        "    \n",
        "  # This dataset already exists as a key, so add this metric\n",
        "  else:\n",
        "    evaluation_dict[dataset].update(\n",
        "        {\n",
        "            'bertscore-precision': avg_precision,\n",
        "            'bertscore-recall': avg_recall,\n",
        "            'bertscore-f1': avg_f1\n",
        "        }\n",
        "    )\n"
      ],
      "metadata": {
        "id": "MtikcvCx1Uw9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### METEOR"
      ],
      "metadata": {
        "id": "mTq3qyEjrC8y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for id, dataset in enumerate(inference_dict.keys()):\n",
        "  print(f\"Evaluating METEOR on {dataset}...\")\n",
        "\n",
        "  targets = inference_dict[dataset]['target'].tolist()\n",
        "  predictions = inference_dict[dataset]['prediction'].tolist()\n",
        "\n",
        "  meteor_results_list = []\n",
        "\n",
        "  # METEOR takes a pair of inputs at a time\n",
        "  for pair in zip(targets, predictions):\n",
        "    \n",
        "    # Calculate METEOR scores\n",
        "    results = meteor.compute(predictions=[pair[0]],\n",
        "                             references=[pair[1]])\n",
        "    \n",
        "    meteor_results_list.append(results['meteor'])\n",
        "  \n",
        "  avg_meteor = sum(meteor_results_list) / len(meteor_results_list)\n",
        "\n",
        "  # Add METEOR to dictionary\n",
        "  # If this dataset hasn't been added to dict, add it and metric\n",
        "  if not evaluation_dict.get(dataset):\n",
        "    evaluation_dict.update(\n",
        "        {\n",
        "            dataset: {'meteor': avg_meteor}\n",
        "        }\n",
        "    )\n",
        "  \n",
        "  # This dataset already exists as a key, so add this metric\n",
        "  else:\n",
        "      evaluation_dict[dataset].update(\n",
        "          {\n",
        "              'meteor': avg_meteor\n",
        "          }\n",
        "      )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhFjML6rrbop",
        "outputId": "c7c3df34-36ea-4690-f951-c1b59589feef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating METEOR on predictions.T5_base_pt.squad.quac.csv...\n",
            "Evaluating METEOR on predictions.T5_base_pt.squad.squad.csv...\n",
            "Evaluating METEOR on predictions.T5_base_pt.quac.squad.csv...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation_dict"
      ],
      "metadata": {
        "id": "0f3trYfXuebr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Archive"
      ],
      "metadata": {
        "id": "SOZSoXny2NR0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "⚠️ This cell takes some time. ⚠️"
      ],
      "metadata": {
        "id": "ZL2ecf5_yWoN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation_dict = {}\n",
        "\n",
        "for id, dataset in enumerate(inference_dict.keys()):\n",
        "\n",
        "  # Get this dataset's `target` and `prediction` values\n",
        "  targets = inference_dict[dataset]['target'].tolist()\n",
        "  predictions = inference_dict[dataset]['prediction'].tolist()\n",
        "\n",
        "  \n",
        "  # Evaluations\n",
        "\n",
        "  # ROUGE\n",
        "  rouge_results = rouge.compute(predictions=predictions,\n",
        "                                references=targets,\n",
        "                                use_aggregator=False)\n",
        "  evaluation_dict.update(\n",
        "      {\n",
        "          dataset: {'rouge': rouge_results}\n",
        "      }\n",
        "  )\n",
        "\n",
        "  # BLEU-RT\n",
        "  bleurt_results = bleurt.compute(predictions=predictions,\n",
        "                                  references=targets)\n",
        "  evaluation_dict.update(\n",
        "      {\n",
        "          dataset: {'bleurt': bleurt_results}\n",
        "      }\n",
        "  )\n",
        "\n",
        "  # BERTScore\n",
        "  bertscore_results = bertscore.compute(predictions=predictions,\n",
        "                                        references=targets,\n",
        "                                        model_type='distilbert-base-uncased')\n",
        "  evaluation_dict.update(\n",
        "      {\n",
        "          dataset: {'bertscore': bleurt_results}\n",
        "      }\n",
        "  )\n",
        "\n",
        "  print(f\"Dataset {dataset} evaluated.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "id": "3axFb4Ihuapi",
        "outputId": "3fee34f9-fee6-48ae-eb28-56e732d8042d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-49-ac0a09b4e6fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m   \u001b[0;31m# BLEU-RT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m   bleurt_results = bleurt.compute(predictions=predictions,\n\u001b[0;32m---> 24\u001b[0;31m                                   references=targets)\n\u001b[0m\u001b[1;32m     25\u001b[0m   evaluation_dict.update(\n\u001b[1;32m     26\u001b[0m       {\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/evaluate/module.py\u001b[0m in \u001b[0;36mcompute\u001b[0;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[1;32m    442\u001b[0m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0minput_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0minput_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtemp_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcompute_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuf_writer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--bleurt/98e148b2f8c4a88aba5037e4e0e90c9fd9ec35dc37a054ded8cfef0fa801ffab/bleurt.py\u001b[0m in \u001b[0;36m_compute\u001b[0;34m(self, predictions, references)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_compute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreferences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscorer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreferences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreferences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandidates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"scores\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/bleurt/score.py\u001b[0m in \u001b[0;36mscore\u001b[0;34m(self, references, candidates, batch_size, *args)\u001b[0m\n\u001b[1;32m    212\u001b[0m           \u001b[0;34m\"segment_ids\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msegment_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m       }\n\u001b[0;32m--> 214\u001b[0;31m       \u001b[0mpredict_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m       \u001b[0mbatch_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m       \u001b[0mall_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/bleurt/score.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, input_dict)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0minput_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_mask\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         segment_ids=tf.constant(input_dict[\"segment_ids\"],\n\u001b[0;32m---> 70\u001b[0;31m                                 dtype=tf.int64))[\"predictions\"].numpy()\n\u001b[0m\u001b[1;32m     71\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1600\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mthe\u001b[0m \u001b[0marguments\u001b[0m \u001b[0mdo\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1601\u001b[0m     \"\"\"\n\u001b[0;32m-> 1602\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1604\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/wrap_function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m       return super(WrappedFunction, self)._call_impl(\n\u001b[0;32m--> 244\u001b[0;31m           args, kwargs, cancellation_manager)\n\u001b[0m\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mprune\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_signature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1618\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mstructured_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_with_flat_signature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_with_flat_signature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_with_flat_signature\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1667\u001b[0m                         \u001b[0;34mf\"#{i}(zero-based) to be a Tensor; \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1668\u001b[0m                         f\"got {type(arg).__name__} ({arg}).\")\n\u001b[0;32m-> 1669\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1671\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_with_structured_signature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1859\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1860\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1861\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1862\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1863\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    500\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 502\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    503\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}