{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Training Seq2Seq models in Colab without running out of RAM\n",
        "\n",
        "This notebook is an extension of the notebook [Training NLP models in Colab without running out of RAM](https://github.com/datasci-w266/2022-fall-main/blob/master/materials/walkthrough_notebooks/keras_with_limited_ram/keras_training_with_limited_ram.ipynb). This one focuses on sequence-to-sequence (encoder-decoder, text generation) models like T5, because the way you fine-tune the Huggingface pretrained versions of those models is a bit different than BERT. With T5, you use the full pre-trained model end-to-end without adding any additional layers.\n",
        "\n",
        "There are a few ways to set up the training for these models, and they differ a little depending on whether you're using tensorflow or pytorch. Huggingface provides trainer classes that can be used to fine-tune their pre-trained models, though these seem to be better supported for pytorch. For tensorflow, the TFTrainer model seems to be deprecated in favor of just using the model's keras functionality (calling .fit). The notebook below shows both how to use a Seq2SeqTrainer for a pytorch model, and how to use keras .fit effectively on a pretrained seq2seq tensorflow model."
      ],
      "metadata": {
        "id": "BJmgxZ_-LcoX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "oMWJ_MH1Lgsg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece"
      ],
      "metadata": {
        "id": "gao8Q8P-edDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "import torch  # Only if you use a pytorch model, both options are shown below\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, TFT5ForConditionalGeneration\n",
        "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer"
      ],
      "metadata": {
        "id": "4fKpWjWlMJZo"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data\n",
        "\n",
        "To fine-tune T5, we'll use the dataset from the [week 6 lesson notebook](https://github.com/datasci-w266/2022-fall-private/blob/master/materials/lesson_notebooks/lesson_6_Machine_Translation.ipynb) for translating Shakespeare to modern English. You can [download the dataset here](https://github.com/cocoxu/Shakespeare), and upload to your drive folder."
      ],
      "metadata": {
        "id": "1VvqKWukMdtV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This cell will authenticate you and mount your Drive in the Colab.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hId0nV1UMJct",
        "outputId": "2c5ad2a0-5445-4932-ca0c-a484f5371d8d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Modify this path to where you saved the Shakespear data in your Drive\n",
        "text_file = 'drive/MyDrive/ISchool/MIDS/W266/2022_Fall/notebooks/train_plays-org-mod.txt'"
      ],
      "metadata": {
        "id": "rG6QmmqWLn6a"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(text_file) as f:\n",
        "    lines = f.read().split('\\n')[:-1]\n",
        "\n",
        "prefix = 'translate old to modern: '\n",
        "text_pairs = []\n",
        "for line in lines:\n",
        "    orig, target = line.split('\\t')\n",
        "    text_pairs.append({'orig': prefix + orig, 'target': target})"
      ],
      "metadata": {
        "id": "t_i8h2C2Mcyd"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Look at some examples\n",
        "for _ in range(5):\n",
        "    print(np.random.choice(text_pairs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lDHlRNR0Mc1D",
        "outputId": "cb601033-777b-43a2-cdcf-788e785001ef"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'orig': 'translate old to modern: What dost thou say?', 'target': 'What did you say?'}\n",
            "{'orig': 'translate old to modern: I may sit in a corner and cry, “Heigh-ho for a husband!” Lady Beatrice, I will get you one.', 'target': 'I should sit in the corner and sing that song, “Heigh-Ho for a Husband!” Lady Beatrice, I’ll get you a husband.'}\n",
            "{'orig': 'translate old to modern: To England will I steal, and there I’ll steal.', 'target': 'I’ll steal away to England, and I’ll steal some more when I get there.'}\n",
            "{'orig': 'translate old to modern: She’s a beagle, true-bred, and one that adores me.', 'target': 'She’s a good little woman, and she adores me.'}\n",
            "{'orig': \"translate old to modern: I did enact Julius Caesar; I was killed i' the Capitol; Brutus killed me.\", 'target': 'I did enact Julius Caesar, I was killed in the Capitol, Brutus killed me.'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's create some splits\n",
        "np.random.shuffle(text_pairs)\n",
        "num_valid_samples = int(0.15 * len(text_pairs))\n",
        "num_train_samples = len(text_pairs) - 2 * num_valid_samples\n",
        "train_pairs = text_pairs[:num_train_samples]\n",
        "valid_pairs = text_pairs[num_train_samples : num_train_samples + num_valid_samples]\n",
        "test_pairs = text_pairs[num_train_samples + num_valid_samples :]\n",
        "\n",
        "print(f\"{len(text_pairs)} total pairs\")\n",
        "print(f\"{len(train_pairs)} training pairs\")\n",
        "print(f\"{len(valid_pairs)} validation pairs\")\n",
        "print(f\"{len(test_pairs)} test pairs\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VGbY61KYMlWo",
        "outputId": "658aeeb6-e7e1-439c-c175-1ddf63f537b2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "19088 total pairs\n",
            "13362 training pairs\n",
            "2863 validation pairs\n",
            "2863 test pairs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save splits to separate csv files, to load only part at a time later\n",
        "train_file = 'drive/MyDrive/ISchool/MIDS/W266/2022_Fall/notebooks/train_pairs.csv'\n",
        "valid_file = 'drive/MyDrive/ISchool/MIDS/W266/2022_Fall/notebooks/valid_pairs.csv'\n",
        "test_file = 'drive/MyDrive/ISchool/MIDS/W266/2022_Fall/notebooks/test_pairs.csv'\n",
        "\n",
        "pd.DataFrame(train_pairs).to_csv(train_file)\n",
        "pd.DataFrame(valid_pairs).to_csv(valid_file)\n",
        "pd.DataFrame(test_pairs).to_csv(test_file)"
      ],
      "metadata": {
        "id": "B3UO4xczMlZe"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Option 1: Pytorch\n",
        "\n",
        "We'll start with pytorch, showing how you can use a Seq2SeqTrainer with a data generator, to control when, how much and how to load your data as you train. The preprocessor and data generator need to be defined slightly differently for the trainer to use.\n",
        "\n",
        "Unlikes the previous notebook, the generator won't load a batch at a time. Instead, the trainer will call our generator (and preprocessing function) for one example at a time. So the preprocessing function needs to return a one-dimensional vector of input_ids for each example, not a two dimensional batch.\n",
        "\n",
        "For the preprocessor, the pytorch models want the inputs in a dictionary with keys for 'input_ids', 'attention_mask', and 'labels'. The first two are the inputs to the encoder (the original text), and the labels are the translated text vocab ids.\n",
        "\n",
        "Since we're passing this all into a trainer, we doon't need to separate out the decoder input_ids. The trainer will infer those from the labels (offset by one)."
      ],
      "metadata": {
        "id": "uq86azvaHG5K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data_pt(text_pair, tokenizer, max_length=128):\n",
        "    orig_text, target_text = text_pair\n",
        "    orig_encoded = tokenizer.batch_encode_plus(\n",
        "        [orig_text],\n",
        "        max_length=max_length,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_attention_mask=True,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "    orig_input_ids = orig_encoded['input_ids'][0]\n",
        "    orig_attention_mask = orig_encoded['attention_mask'][0]\n",
        "    \n",
        "    target_encoded = tokenizer.batch_encode_plus(\n",
        "        [target_text],\n",
        "        max_length=max_length,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_attention_mask=True,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "    \n",
        "    label_ids = target_encoded['input_ids'][0]\n",
        "    \n",
        "    return {'input_ids': orig_input_ids,\n",
        "            'attention_mask': orig_attention_mask,\n",
        "            'labels': label_ids}"
      ],
      "metadata": {
        "id": "0VEFHBNBHF-C"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TranslationDataGeneratorPT(tf.keras.utils.Sequence):\n",
        "    \n",
        "    def __init__(self,\n",
        "                 tokenizer,\n",
        "                 n_examples,\n",
        "                 data_filename,\n",
        "                 max_length=128,\n",
        "                 shuffle=True):\n",
        "        \n",
        "        self.tokenizer = tokenizer\n",
        "        self.n_examples = n_examples\n",
        "        self.data_filename = data_filename\n",
        "        self.max_length = max_length\n",
        "        self.shuffle = shuffle\n",
        "        \n",
        "        # Initialize row order, call on_epoch_end to shuffle row indices\n",
        "        self.row_order = np.arange(1, self.n_examples+1)\n",
        "        self.on_epoch_end()\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.n_examples\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        row_to_load = self.row_order[idx]\n",
        "        df = pd.read_csv(self.data_filename,\n",
        "                         skiprows=range(1, row_to_load),\n",
        "                         nrows=1)\n",
        "        \n",
        "        text_pairs = df[['orig', 'target']].values.astype(str)[0]\n",
        "        \n",
        "        batch_data = preprocess_data_pt(\n",
        "            text_pairs,\n",
        "            self.tokenizer,\n",
        "            self.max_length\n",
        "        )\n",
        "\n",
        "        return batch_data\n",
        "    \n",
        "    def __call__(self):\n",
        "        for i in range(self.__len__()):\n",
        "            yield self.__getitem__(i)\n",
        "            \n",
        "            if i == self.__len__()-1:\n",
        "                self.on_epoch_end()\n",
        "    \n",
        "    def on_epoch_end(self):\n",
        "        if self.shuffle:\n",
        "            self.row_order = list(np.random.permutation(self.row_order))"
      ],
      "metadata": {
        "id": "aTahX_whHGAq"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download tokenizer and model\n",
        "\n",
        "model_name = 't5-base'\n",
        "t5_tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "t5_model_pt = T5ForConditionalGeneration.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "BfK_F7p5P5ir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the data generators for train and validation data, pytorch version\n",
        "\n",
        "max_length = 32\n",
        "\n",
        "train_data_generator = TranslationDataGeneratorPT(\n",
        "    tokenizer=t5_tokenizer,\n",
        "    n_examples=len(train_pairs),\n",
        "    data_filename=train_file,\n",
        "    max_length=max_length\n",
        ")\n",
        "\n",
        "valid_data_generator = TranslationDataGeneratorPT(\n",
        "    tokenizer=t5_tokenizer,\n",
        "    n_examples=len(valid_pairs),\n",
        "    data_filename=valid_file,\n",
        "    max_length=max_length"
      ],
      "metadata": {
        "id": "FuSczzguHGDO"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify batch size and other training arguments\n",
        "\n",
        "batch_size = 16\n",
        "\n",
        "# Modify this filepath to where you want to save the model after fine-tuning\n",
        "dir_path = 'drive/MyDrive/ISchool/MIDS/W266/2022_Fall/notebooks/'\n",
        "file_path = dir_path + 't5base-finetuned-shakespeare-to-modern'\n",
        "\n",
        "args = Seq2SeqTrainingArguments(\n",
        "    file_path,\n",
        "    evaluation_strategy='epoch',\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    num_train_epochs=1,\n",
        ")"
      ],
      "metadata": {
        "id": "QHGnHso9bSbI"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the trainer, passing in the model, training args, and data generators\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    t5_model_pt,\n",
        "    args,\n",
        "    train_dataset=train_data_generator,\n",
        "    eval_dataset=valid_data_generator\n",
        ")"
      ],
      "metadata": {
        "id": "zx4fOPVJLoLn"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Call train\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        },
        "id": "QF-x_gTibIN2",
        "outputId": "a8a2a967-8c4a-4b45-a656-8155219cbef6"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 13362\n",
            "  Num Epochs = 1\n",
            "  Instantaneous batch size per device = 16\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 836\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='836' max='836' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [836/836 08:11, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.995000</td>\n",
              "      <td>0.695925</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to drive/MyDrive/ISchool/MIDS/W266/2022_Summer/notebooks/t5base-finetuned-old-to-modern/checkpoint-500\n",
            "Configuration saved in drive/MyDrive/ISchool/MIDS/W266/2022_Summer/notebooks/t5base-finetuned-old-to-modern/checkpoint-500/config.json\n",
            "Model weights saved in drive/MyDrive/ISchool/MIDS/W266/2022_Summer/notebooks/t5base-finetuned-old-to-modern/checkpoint-500/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2863\n",
            "  Batch size = 16\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=836, training_loss=0.9100153799832723, metrics={'train_runtime': 497.2823, 'train_samples_per_second': 26.87, 'train_steps_per_second': 1.681, 'total_flos': 508555958353920.0, 'train_loss': 0.9100153799832723, 'epoch': 1.0})"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Does it seem to have worked?\n",
        "\n",
        "Depending on your task, you'll add your own model evaluation after training. Here's a simple check to make sure it does seem to have fine-tuned T5 for this new task we defined."
      ],
      "metadata": {
        "id": "oYwGhRWsnZj_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for test_input_text in ['Hence forth thou shalt not vex me e\\'er again.',\n",
        "                        'Dost thou foresake me?',\n",
        "                        'Makest thine own dinner.']:\n",
        "    test_inputs = t5_tokenizer([prefix + test_input_text], return_tensors='pt')\n",
        "    test_output_ids = t5_model_pt.generate(test_inputs['input_ids'].cuda())\n",
        "\n",
        "    print([t5_tokenizer.decode(out_ids, skip_special_tokens=True, \n",
        "                              clean_up_tokenization_spaces=False) for out_ids in test_output_ids])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RMRlS4LqbIaf",
        "outputId": "1d11210a-259e-4537-b9e1-11ebc7298ec9"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/generation_utils.py:1301: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 20 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['You’ll not vex me again.']\n",
            "['Do you foresake me?']\n",
            "['Make your own dinner.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can load the model you trained using the .from_pretrained function you use for pretrained models. If you look in your drive folder, at the filepath you used in the trainer arguments, you'll see a checkpoint folder. Use the full path to that checkpoint folder as the argument to .from_pretrained, to load the model you saved again later."
      ],
      "metadata": {
        "id": "NffN-QTEQV17"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "t5_model_saved = T5ForConditionalGeneration.from_pretrained(file_path + '/checkpoint-500')"
      ],
      "metadata": {
        "id": "4AKDOxmxzatj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Still works?\n",
        "for test_input_text in ['Hence forth thou shalt not vex me e\\'er again.',\n",
        "                        'Dost thou foresake me?',\n",
        "                        'Makest thine own dinner.']:\n",
        "    test_inputs = t5_tokenizer([prefix + test_input_text], return_tensors='pt')\n",
        "    test_output_ids = t5_model_saved.generate(test_inputs['input_ids'])\n",
        "\n",
        "    print([t5_tokenizer.decode(out_ids, skip_special_tokens=True, \n",
        "                              clean_up_tokenization_spaces=False) for out_ids in test_output_ids])"
      ],
      "metadata": {
        "id": "xR1_HTh3zavY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a1470f0-b4e5-42f2-973d-561c4ea8447e"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/generation_utils.py:1301: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 20 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['You’ll not vex me again.']\n",
            "['Do you want to leave me?']\n",
            "['Make your own dinner.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Option 2: Tensorflow\n",
        "\n",
        "For tensorflow, Huggingface seems to have deprecated their TFTrainer in favor of using keras .fit. You can call .compile() and .fit() directly on the pre-trained T5 model, but it can be tricky to make sure the right inputs are going into the right part of the model, since tensorflow models take a separate list of inputs and labels, usually not in a dictionary with keys like the pytorch version.\n",
        "\n",
        "Even though we aren't adding any other layers, we can still create a keras model wrapper around the pretrained T5 model. That way, we can pass in the right inputs into the model using keyword arguments. In this case, we'll not only pass in the input_ids and attention_mask for the encoder (original text), we'll also need to pass in the decoder_input_ids. The T5 model has a handy function to shift the labels over by one, so they start with the starter token for the decoder inputs.\n",
        "\n",
        "We'll just use the first output of the T5 model (the logits) as the output of the overall model, and compile with crossentropy loss. Then we can call .fit on the wrapper model like we did in the last notebook, using a data generator that loads a batch of data each time."
      ],
      "metadata": {
        "id": "c9aloMo7QnBG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(text_pairs, tokenizer, model, max_length=128):\n",
        "    orig_text = [orig for orig, target in text_pairs]\n",
        "    orig_encoded = tokenizer.batch_encode_plus(\n",
        "        orig_text,\n",
        "        max_length=max_length,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_attention_mask=True,\n",
        "        return_tensors=\"tf\"\n",
        "    )\n",
        "\n",
        "    orig_input_ids = np.array(orig_encoded[\"input_ids\"], dtype=\"int32\")\n",
        "    orig_attention_masks = np.array(orig_encoded[\"attention_mask\"], dtype=\"int32\")\n",
        "    \n",
        "    target_text = [target for orig, target in text_pairs]\n",
        "    target_encoded = tokenizer.batch_encode_plus(\n",
        "        target_text,\n",
        "        max_length=max_length,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_tensors=\"tf\"\n",
        "    )\n",
        "\n",
        "    label_ids = np.array(target_encoded['input_ids'])\n",
        "    decoder_input_ids = model._shift_right(label_ids)\n",
        "    \n",
        "    return [orig_input_ids, orig_attention_masks, decoder_input_ids], label_ids"
      ],
      "metadata": {
        "id": "ZiSOsXovGwb7"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TranslationDataGenerator(tf.keras.utils.Sequence):\n",
        "    \n",
        "    def __init__(self,\n",
        "                 tokenizer,\n",
        "                 model,\n",
        "                 n_examples,\n",
        "                 data_filename,\n",
        "                 max_length=128,\n",
        "                 batch_size=16,\n",
        "                 shuffle=True):\n",
        "        \n",
        "        self.tokenizer = tokenizer\n",
        "        self.model = model\n",
        "        self.n_examples = n_examples\n",
        "        self.data_filename = data_filename\n",
        "        self.max_length = max_length\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "        \n",
        "        # Initialize row order, call on_epoch_end to shuffle row indices\n",
        "        self.row_order = np.arange(1, self.n_examples+1)\n",
        "        self.on_epoch_end()\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.n_examples // self.batch_size\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        batch_start = idx * self.batch_size\n",
        "        batch_end = (idx + 1) * self.batch_size\n",
        "\n",
        "        # Indices to skip are the ones in the shuffled row_order before and\n",
        "        # after the chunk we'll use for this batch\n",
        "        batch_idx_skip = self.row_order[:batch_start] + self.row_order[batch_end:]\n",
        "        df = pd.read_csv(self.data_filename, skiprows=batch_idx_skip)\n",
        "        \n",
        "        text_pairs = df[['orig', 'target']].values.astype(str).tolist()\n",
        "        \n",
        "        batch_data = preprocess_data(\n",
        "            text_pairs,\n",
        "            self.tokenizer,\n",
        "            self.model,\n",
        "            self.max_length\n",
        "        )\n",
        "\n",
        "        return batch_data\n",
        "    \n",
        "    def __call__(self):\n",
        "        for i in range(self.__len__()):\n",
        "            yield self.__getitem__(i)\n",
        "            \n",
        "            if i == self.__len__()-1:\n",
        "                self.on_epoch_end()\n",
        "    \n",
        "    def on_epoch_end(self):\n",
        "        if self.shuffle:\n",
        "            self.row_order = list(np.random.permutation(self.row_order))"
      ],
      "metadata": {
        "id": "ZoW-A-ENSJ3J"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pretrained tensorflow model\n",
        "\n",
        "model_name = 't5-base'\n",
        "t5_tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "t5_model_tf = TFT5ForConditionalGeneration.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "lfznBi6kBM6z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the data generators for train and validation data, tensorflow version\n",
        "\n",
        "max_length = 32\n",
        "batch_size = 16\n",
        "\n",
        "train_data_generator = TranslationDataGenerator(\n",
        "    tokenizer=t5_tokenizer,\n",
        "    model=t5_model_tf,\n",
        "    n_examples=len(train_pairs),\n",
        "    data_filename=train_file,\n",
        "    max_length=max_length,\n",
        "    batch_size=batch_size\n",
        ")\n",
        "\n",
        "valid_data_generator = TranslationDataGenerator(\n",
        "    tokenizer=t5_tokenizer,\n",
        "    model=t5_model_tf,\n",
        "    n_examples=len(valid_pairs),\n",
        "    data_filename=valid_file,\n",
        "    max_length=max_length,\n",
        "    batch_size=batch_size\n",
        ")"
      ],
      "metadata": {
        "id": "WExN1Fp3SJ5c"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_t5_training_wrapper_model(t5_model, max_length):\n",
        "    input_ids = layers.Input(shape=(max_length), dtype=tf.int32, name='input_ids')\n",
        "    attention_mask = layers.Input(shape=(max_length), dtype=tf.int32, name='attention_masks')\n",
        "    decoder_input_ids = layers.Input(shape=(max_length), dtype=tf.int32, name='labels')\n",
        "    \n",
        "    t5_logits = t5_model(input_ids, attention_mask=attention_mask, decoder_input_ids=decoder_input_ids)[0]\n",
        "\n",
        "    model = tf.keras.models.Model(inputs=[input_ids, attention_mask, decoder_input_ids],\n",
        "                                  outputs=[t5_logits])\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
        "                  loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                  metrics=['accuracy'])\n",
        "    \n",
        "    return model"
      ],
      "metadata": {
        "id": "ZacUWKNwrMcU"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_wrapper = build_t5_training_wrapper_model(t5_model_tf, max_length)"
      ],
      "metadata": {
        "id": "ZXRCuarjrMen"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# As in the first notebook, we should add a model checkpoint callback to save\n",
        "# the trained model weights after each epoch. Edit the filepath to where\n",
        "# you want to save the weights in your own Drive\n",
        "\n",
        "checkpoint_dir = 'drive/MyDrive/ISchool/MIDS/W266/2022_Fall/notebooks/model_checkpoints/'\n",
        "checkpoint_filepath = checkpoint_dir + 't5_shakespeare_weights.{epoch:02d}-{val_accuracy:.2f}.hdf5'\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_weights_only=True)"
      ],
      "metadata": {
        "id": "VDh-uMYG8jLe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now call .fit on the model_wrapper, passing in the data generators and the\n",
        "# model checkpoint callback\n",
        "\n",
        "model_wrapper.fit(train_data_generator,\n",
        "                  validation_data=valid_data_generator,\n",
        "                  epochs=1,\n",
        "                  callbacks=[model_checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5KkhVMODrMg9",
        "outputId": "a0d079af-7164-4a59-9a31-ac83e690517e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "835/835 [==============================] - 306s 329ms/step - loss: 0.8334 - accuracy: 0.8391 - val_loss: 0.7253 - val_accuracy: 0.8541\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f9e102eb290>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Does it work?\n",
        "\n",
        "Again, depending on your task, you'll add your own model evaluation after training. Here's a simple check to make sure it does seem to have fine-tuned T5 for this new task we defined."
      ],
      "metadata": {
        "id": "D9NGPOrhUbRC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for test_input_text in ['Hence forth thou shalt not vex me e\\'er again.',\n",
        "                        'Dost thou foresake me?',\n",
        "                        'Makest thine own dinner.']:\n",
        "    test_inputs = t5_tokenizer([prefix + test_input_text], return_tensors='pt')\n",
        "    test_output_ids = t5_model_tf.generate(test_inputs['input_ids'])\n",
        "\n",
        "    print([t5_tokenizer.decode(out_ids, skip_special_tokens=True, \n",
        "                              clean_up_tokenization_spaces=False) for out_ids in test_output_ids])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ky4-uaxGweg",
        "outputId": "3bad50fe-fb48-4c5a-feed-58e4b8576eb2"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['You’re not going to bother me again.']\n",
            "['Do you leave me?']\n",
            "['Make your own dinner.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To pick back up where you left off, load the saved model weights\n",
        "# (Edit the filename to the last saved one that you want to load)\n",
        "\n",
        "checkpoint_filepath = checkpoint_dir + 't5_shakespeare_weights.01-0.85.hdf5'\n",
        "model_wrapper.load_weights(checkpoint_filepath)"
      ],
      "metadata": {
        "id": "GzuZEB6EUa7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Still works?\n",
        "for test_input_text in ['Hence forth thou shalt not vex me e\\'er again.',\n",
        "                        'Dost thou foresake me?',\n",
        "                        'Makest thine own dinner.']:\n",
        "    test_inputs = t5_tokenizer([prefix + test_input_text], return_tensors='pt')\n",
        "    test_output_ids = t5_model_tf.generate(test_inputs['input_ids'])\n",
        "\n",
        "    print([t5_tokenizer.decode(out_ids, skip_special_tokens=True, \n",
        "                              clean_up_tokenization_spaces=False) for out_ids in test_output_ids])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qFxYoN-kC133",
        "outputId": "45f902e4-d021-4679-9b1b-b6a096b53792"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/generation_tf_utils.py:1695: UserWarning: Neither `max_length` nor `max_new_tokens` have been set, `max_length` will default to 20 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['You’re not going to bother me again.']\n",
            "['Do you leave me?']\n",
            "['Make your own dinner.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "55l1GRNtC5P8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}