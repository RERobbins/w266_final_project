{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "oYwGhRWsnZj_",
        "c9aloMo7QnBG",
        "D9NGPOrhUbRC"
      ],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeanlucjackson/w266_final_project/blob/main/code/RR/question_generation_t5_with_data_generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question Generation using T5 in Colab without running out of RAM\n",
        "\n",
        "This notebook is based on Natalie Ahn's notebook showing how to fine tune T5 in Colab without running out of RAM.\n",
        "\n",
        "It includes pytorch and tensorflow examples.\n",
        "\n",
        "We use the SQUAD dataset."
      ],
      "metadata": {
        "id": "BJmgxZ_-LcoX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers"
      ],
      "metadata": {
        "id": "oMWJ_MH1Lgsg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q sentencepiece"
      ],
      "metadata": {
        "id": "gao8Q8P-edDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q datasets"
      ],
      "metadata": {
        "id": "BqpebI92dpsj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "from pprint import pprint\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "import torch  # Only if you use a pytorch model, both options are shown below\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, TFT5ForConditionalGeneration\n",
        "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "\n",
        "from datasets import list_datasets, load_dataset_builder, get_dataset_config_names, load_dataset, load_from_disk\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)"
      ],
      "metadata": {
        "id": "4fKpWjWlMJZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This cell will authenticate you and mount your Drive in the Colab.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "hId0nV1UMJct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_dataset (dataset, config=None):\n",
        "    builder = load_dataset_builder(dataset, config)\n",
        "    pprint(f\"Description:\\n {builder.info.description}\")\n",
        "    print(f\"Features:\")\n",
        "    pprint(builder.info.features)\n",
        "    return"
      ],
      "metadata": {
        "id": "i_g15ELCggxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data\n",
        "\n",
        "We use SQUAD."
      ],
      "metadata": {
        "id": "1VvqKWukMdtV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_root = \"/content/drive/MyDrive/w266 NLP Final Project/Data/\"\n",
        "dataset_name = \"squad\"\n",
        "dataset_folder = dataset_root+dataset_name+\".hf\""
      ],
      "metadata": {
        "id": "YoBb_fmzlitt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Begin with a dataset summary.\n",
        "summarize_dataset(dataset_name)"
      ],
      "metadata": {
        "id": "yLvobzM_gNcZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Load the data"
      ],
      "metadata": {
        "id": "FTSVZdFIrRh-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SQuAD is quick to download from Hugging Face\n",
        "# Use the code below if you aren't accessing the data from the shared\n",
        "# Google Drive folder.\n",
        "\n",
        "# dataset = load_dataset(dataset_name)\n",
        "\n",
        "# The followind code assumes you have added a link to the shared \n",
        "# w266 NLP Final Project folder in your Google Drive folder\n",
        "# Loading data from there is faster.\n",
        "\n",
        "dataset = load_from_disk(dataset_folder)"
      ],
      "metadata": {
        "id": "zVL1tMOZgNky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data_squad.save_to_disk(dataset_folder)"
      ],
      "metadata": {
        "id": "eu3wqZt0gNqT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "C7L7K27IgNtA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_data = dataset['train'].shuffle(seed=1962)"
      ],
      "metadata": {
        "id": "iCRt4dFhjZfx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "validation_data = dataset['validation'].shuffle(seed=1962)"
      ],
      "metadata": {
        "id": "y85KRqDSjZix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_answers = [answer['text'][0] for answer in training_data['answers']]\n",
        "training_context = training_data['context']\n",
        "training_questions = training_data['question']"
      ],
      "metadata": {
        "id": "u4EPeKcTjZuY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "validation_answers = [answer['text'][0] for answer in validation_data['answers']]\n",
        "validation_context = validation_data['context']\n",
        "validation_questions = validation_data['question']"
      ],
      "metadata": {
        "id": "YKO3tax5jZxQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Assemble input and output pairs\n",
        "\n",
        "The input format is:\n",
        "generate question: answer: XXXXXXX context: XXXXXXXX"
      ],
      "metadata": {
        "id": "lT8nnropsUZM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_orig = [f\"generate question: answer: {answer} context: {context}\" for answer, context in zip (training_answers, training_context)]\n",
        "training_target = training_questions\n",
        "validiation_orig = [f\"generate question: answer: {answer} context: {context}\" for answer, context in zip (validation_answers, validation_context)]\n",
        "validation_target = validation_questions"
      ],
      "metadata": {
        "id": "lAqS8aQ5RJ-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_df = pd.DataFrame()\n",
        "training_df['orig'] = training_orig\n",
        "training_df['target'] = training_target\n",
        "training_df"
      ],
      "metadata": {
        "id": "mfgzW6pZSLdq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "validation_df = pd.DataFrame()\n",
        "validation_df['orig'] = validiation_orig\n",
        "validation_df['target'] = validation_target\n",
        "validation_df"
      ],
      "metadata": {
        "id": "OxzzyOm7VKoY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### This call is a work in progress, for now let's rely on what we did above....\n",
        "\n",
        "# Let's create some splits\n",
        "#np.random.shuffle(text_pairs)\n",
        "#num_valid_samples = int(0.15 * len(text_pairs))\n",
        "#num_train_samples = len(text_pairs) - 2 * num_valid_samples\n",
        "train_pairs = training_df.shape[0]\n",
        "valid_pairs = validation_df.shape[0]\n",
        "#test_pairs = text_pairs[num_train_samples + num_valid_samples :]\n",
        "\n",
        "#print(f\"{len(text_pairs)} total pairs\")\n",
        "print(f\"{train_pairs} training pairs\")\n",
        "print(f\"{valid_pairs} validation pairs\")\n",
        "#print(f\"{len(test_pairs)} test pairs\")"
      ],
      "metadata": {
        "id": "VGbY61KYMlWo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save splits to separate csv files, to load only part at a time later\n",
        "training_file = dataset_folder + '/train_pairs.csv'\n",
        "validation_file = dataset_folder + '/valid_pairs.csv'\n",
        "# test_file = 'drive/MyDrive/ISchool/MIDS/W266/2022_Fall/notebooks/test_pairs.csv'\n",
        "\n",
        "training_df.to_csv(training_file)\n",
        "validation_df.to_csv(validation_file)\n",
        "# pd.DataFrame(test_pairs).to_csv(test_file)"
      ],
      "metadata": {
        "id": "B3UO4xczMlZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(training_file)\n",
        "df"
      ],
      "metadata": {
        "id": "BVKtkZfxIj9N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Option 1: Pytorch\n",
        "\n",
        "We'll start with pytorch, showing how you can use a Seq2SeqTrainer with a data generator, to control when, how much and how to load your data as you train. The preprocessor and data generator need to be defined slightly differently for the trainer to use.\n",
        "\n",
        "Unlikes the previous notebook, the generator won't load a batch at a time. Instead, the trainer will call our generator (and preprocessing function) for one example at a time. So the preprocessing function needs to return a one-dimensional vector of input_ids for each example, not a two dimensional batch.\n",
        "\n",
        "For the preprocessor, the pytorch models want the inputs in a dictionary with keys for 'input_ids', 'attention_mask', and 'labels'. The first two are the inputs to the encoder (the original text), and the labels are the translated text vocab ids.\n",
        "\n",
        "Since we're passing this all into a trainer, we doon't need to separate out the decoder input_ids. The trainer will infer those from the labels (offset by one)."
      ],
      "metadata": {
        "id": "uq86azvaHG5K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data_pt(text_pair, tokenizer, max_length=1024):\n",
        "    orig_text, target_text = text_pair\n",
        "    orig_encoded = tokenizer.batch_encode_plus(\n",
        "        [orig_text],\n",
        "        max_length=max_length,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_attention_mask=True,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "    orig_input_ids = orig_encoded['input_ids'][0]\n",
        "    orig_attention_mask = orig_encoded['attention_mask'][0]\n",
        "    \n",
        "    target_encoded = tokenizer.batch_encode_plus(\n",
        "        [target_text],\n",
        "        max_length=max_length,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_attention_mask=True,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "    \n",
        "    label_ids = target_encoded['input_ids'][0]\n",
        "    \n",
        "    return {'input_ids': orig_input_ids,\n",
        "            'attention_mask': orig_attention_mask,\n",
        "            'labels': label_ids}"
      ],
      "metadata": {
        "id": "0VEFHBNBHF-C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TranslationDataGeneratorPT(tf.keras.utils.Sequence):\n",
        "    \n",
        "    def __init__(self,\n",
        "                 tokenizer,\n",
        "                 n_examples,\n",
        "                 data_filename,\n",
        "                 max_length=1024,\n",
        "                 shuffle=True):\n",
        "        \n",
        "        self.tokenizer = tokenizer\n",
        "        self.n_examples = n_examples\n",
        "        self.data_filename = data_filename\n",
        "        self.max_length = max_length\n",
        "        self.shuffle = shuffle\n",
        "        \n",
        "        # Initialize row order, call on_epoch_end to shuffle row indices\n",
        "        self.row_order = np.arange(1, self.n_examples+1)\n",
        "        self.on_epoch_end()\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.n_examples\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        row_to_load = self.row_order[idx]\n",
        "        df = pd.read_csv(self.data_filename,\n",
        "                         skiprows=range(1, row_to_load),\n",
        "                         nrows=1)\n",
        "        \n",
        "        text_pairs = df[['orig', 'target']].values.astype(str)[0]\n",
        "        \n",
        "        batch_data = preprocess_data_pt(\n",
        "            text_pairs,\n",
        "            self.tokenizer,\n",
        "            self.max_length\n",
        "        )\n",
        "\n",
        "        return batch_data\n",
        "    \n",
        "    def __call__(self):\n",
        "        for i in range(self.__len__()):\n",
        "            yield self.__getitem__(i)\n",
        "            \n",
        "            if i == self.__len__()-1:\n",
        "                self.on_epoch_end()\n",
        "    \n",
        "    def on_epoch_end(self):\n",
        "        if self.shuffle:\n",
        "            self.row_order = list(np.random.permutation(self.row_order))"
      ],
      "metadata": {
        "id": "aTahX_whHGAq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download tokenizer and model\n",
        "\n",
        "model_name = \"google/t5-v1_1-base\"\n",
        "t5_tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "t5_model_pt = T5ForConditionalGeneration.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "BfK_F7p5P5ir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the data generators for train and validation data, pytorch version\n",
        "\n",
        "max_length = 1024\n",
        "\n",
        "train_data_generator = TranslationDataGeneratorPT(\n",
        "    tokenizer=t5_tokenizer,\n",
        "    n_examples=training_df.shape[0],\n",
        "    data_filename=training_file,\n",
        "    max_length=max_length\n",
        ")\n",
        "\n",
        "valid_data_generator = TranslationDataGeneratorPT(\n",
        "    tokenizer=t5_tokenizer,\n",
        "    n_examples=validation_df.shape[0],\n",
        "    data_filename=validation_file,\n",
        "    max_length=max_length\n",
        ")"
      ],
      "metadata": {
        "id": "FuSczzguHGDO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify batch size and other training arguments\n",
        "\n",
        "batch_size = 16\n",
        "\n",
        "# Modify this filepath to where you want to save the model after fine-tuning\n",
        "dir_path = \"/content/drive/MyDrive/w266 NLP Final Project/Models/RR Squad One/\"\n",
        "file_path = dir_path + 't5base-finetuned-squad'\n",
        "\n",
        "args = Seq2SeqTrainingArguments(\n",
        "    file_path,\n",
        "    evaluation_strategy='epoch',\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    num_train_epochs=1,\n",
        ")"
      ],
      "metadata": {
        "id": "QHGnHso9bSbI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the trainer, passing in the model, training args, and data generators\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    t5_model_pt,\n",
        "    args,\n",
        "    train_dataset=train_data_generator,\n",
        "    eval_dataset=valid_data_generator\n",
        ")"
      ],
      "metadata": {
        "id": "zx4fOPVJLoLn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Call train\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "QF-x_gTibIN2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Does it seem to have worked?\n",
        "\n",
        "Depending on your task, you'll add your own model evaluation after training. Here's a simple check to make sure it does seem to have fine-tuned T5 for this new task we defined."
      ],
      "metadata": {
        "id": "oYwGhRWsnZj_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for test_input_text in ['Hence forth thou shalt not vex me e\\'er again.',\n",
        "                        'Dost thou foresake me?',\n",
        "                        'Makest thine own dinner.']:\n",
        "    test_inputs = t5_tokenizer([prefix + test_input_text], return_tensors='pt')\n",
        "    test_output_ids = t5_model_pt.generate(test_inputs['input_ids'].cuda())\n",
        "\n",
        "    print([t5_tokenizer.decode(out_ids, skip_special_tokens=True, \n",
        "                              clean_up_tokenization_spaces=False) for out_ids in test_output_ids])"
      ],
      "metadata": {
        "id": "RMRlS4LqbIaf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can load the model you trained using the .from_pretrained function you use for pretrained models. If you look in your drive folder, at the filepath you used in the trainer arguments, you'll see a checkpoint folder. Use the full path to that checkpoint folder as the argument to .from_pretrained, to load the model you saved again later."
      ],
      "metadata": {
        "id": "NffN-QTEQV17"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "t5_model_saved = T5ForConditionalGeneration.from_pretrained(file_path + '/checkpoint-500')"
      ],
      "metadata": {
        "id": "4AKDOxmxzatj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Still works?\n",
        "for test_input_text in ['Hence forth thou shalt not vex me e\\'er again.',\n",
        "                        'Dost thou foresake me?',\n",
        "                        'Makest thine own dinner.']:\n",
        "    test_inputs = t5_tokenizer([prefix + test_input_text], return_tensors='pt')\n",
        "    test_output_ids = t5_model_saved.generate(test_inputs['input_ids'])\n",
        "\n",
        "    print([t5_tokenizer.decode(out_ids, skip_special_tokens=True, \n",
        "                              clean_up_tokenization_spaces=False) for out_ids in test_output_ids])"
      ],
      "metadata": {
        "id": "xR1_HTh3zavY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Option 2: Tensorflow\n",
        "\n",
        "For tensorflow, Huggingface seems to have deprecated their TFTrainer in favor of using keras .fit. You can call .compile() and .fit() directly on the pre-trained T5 model, but it can be tricky to make sure the right inputs are going into the right part of the model, since tensorflow models take a separate list of inputs and labels, usually not in a dictionary with keys like the pytorch version.\n",
        "\n",
        "Even though we aren't adding any other layers, we can still create a keras model wrapper around the pretrained T5 model. That way, we can pass in the right inputs into the model using keyword arguments. In this case, we'll not only pass in the input_ids and attention_mask for the encoder (original text), we'll also need to pass in the decoder_input_ids. The T5 model has a handy function to shift the labels over by one, so they start with the starter token for the decoder inputs.\n",
        "\n",
        "We'll just use the first output of the T5 model (the logits) as the output of the overall model, and compile with crossentropy loss. Then we can call .fit on the wrapper model like we did in the last notebook, using a data generator that loads a batch of data each time."
      ],
      "metadata": {
        "id": "c9aloMo7QnBG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(text_pairs, tokenizer, model, max_length=128):\n",
        "    orig_text = [orig for orig, target in text_pairs]\n",
        "    orig_encoded = tokenizer.batch_encode_plus(\n",
        "        orig_text,\n",
        "        max_length=max_length,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_attention_mask=True,\n",
        "        return_tensors=\"tf\"\n",
        "    )\n",
        "\n",
        "    orig_input_ids = np.array(orig_encoded[\"input_ids\"], dtype=\"int32\")\n",
        "    orig_attention_masks = np.array(orig_encoded[\"attention_mask\"], dtype=\"int32\")\n",
        "    \n",
        "    target_text = [target for orig, target in text_pairs]\n",
        "    target_encoded = tokenizer.batch_encode_plus(\n",
        "        target_text,\n",
        "        max_length=max_length,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_tensors=\"tf\"\n",
        "    )\n",
        "\n",
        "    label_ids = np.array(target_encoded['input_ids'])\n",
        "    decoder_input_ids = model._shift_right(label_ids)\n",
        "    \n",
        "    return [orig_input_ids, orig_attention_masks, decoder_input_ids], label_ids"
      ],
      "metadata": {
        "id": "ZiSOsXovGwb7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TranslationDataGenerator(tf.keras.utils.Sequence):\n",
        "    \n",
        "    def __init__(self,\n",
        "                 tokenizer,\n",
        "                 model,\n",
        "                 n_examples,\n",
        "                 data_filename,\n",
        "                 max_length=128,\n",
        "                 batch_size=16,\n",
        "                 shuffle=True):\n",
        "        \n",
        "        self.tokenizer = tokenizer\n",
        "        self.model = model\n",
        "        self.n_examples = n_examples\n",
        "        self.data_filename = data_filename\n",
        "        self.max_length = max_length\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "        \n",
        "        # Initialize row order, call on_epoch_end to shuffle row indices\n",
        "        self.row_order = np.arange(1, self.n_examples+1)\n",
        "        self.on_epoch_end()\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.n_examples // self.batch_size\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        batch_start = idx * self.batch_size\n",
        "        batch_end = (idx + 1) * self.batch_size\n",
        "\n",
        "        # Indices to skip are the ones in the shuffled row_order before and\n",
        "        # after the chunk we'll use for this batch\n",
        "        batch_idx_skip = self.row_order[:batch_start] + self.row_order[batch_end:]\n",
        "        df = pd.read_csv(self.data_filename, skiprows=batch_idx_skip)\n",
        "        \n",
        "        text_pairs = df[['orig', 'target']].values.astype(str).tolist()\n",
        "        \n",
        "        batch_data = preprocess_data(\n",
        "            text_pairs,\n",
        "            self.tokenizer,\n",
        "            self.model,\n",
        "            self.max_length\n",
        "        )\n",
        "\n",
        "        return batch_data\n",
        "    \n",
        "    def __call__(self):\n",
        "        for i in range(self.__len__()):\n",
        "            yield self.__getitem__(i)\n",
        "            \n",
        "            if i == self.__len__()-1:\n",
        "                self.on_epoch_end()\n",
        "    \n",
        "    def on_epoch_end(self):\n",
        "        if self.shuffle:\n",
        "            self.row_order = list(np.random.permutation(self.row_order))"
      ],
      "metadata": {
        "id": "ZoW-A-ENSJ3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pretrained tensorflow model\n",
        "\n",
        "model_name = 't5-base'\n",
        "t5_tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "t5_model_tf = TFT5ForConditionalGeneration.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "lfznBi6kBM6z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the data generators for train and validation data, tensorflow version\n",
        "\n",
        "max_length = 32\n",
        "batch_size = 16\n",
        "\n",
        "train_data_generator = TranslationDataGenerator(\n",
        "    tokenizer=t5_tokenizer,\n",
        "    model=t5_model_tf,\n",
        "    n_examples=len(train_pairs),\n",
        "    data_filename=train_file,\n",
        "    max_length=max_length,\n",
        "    batch_size=batch_size\n",
        ")\n",
        "\n",
        "valid_data_generator = TranslationDataGenerator(\n",
        "    tokenizer=t5_tokenizer,\n",
        "    model=t5_model_tf,\n",
        "    n_examples=len(valid_pairs),\n",
        "    data_filename=valid_file,\n",
        "    max_length=max_length,\n",
        "    batch_size=batch_size\n",
        ")"
      ],
      "metadata": {
        "id": "WExN1Fp3SJ5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_t5_training_wrapper_model(t5_model, max_length):\n",
        "    input_ids = layers.Input(shape=(max_length), dtype=tf.int32, name='input_ids')\n",
        "    attention_mask = layers.Input(shape=(max_length), dtype=tf.int32, name='attention_masks')\n",
        "    decoder_input_ids = layers.Input(shape=(max_length), dtype=tf.int32, name='labels')\n",
        "    \n",
        "    t5_logits = t5_model(input_ids, attention_mask=attention_mask, decoder_input_ids=decoder_input_ids)[0]\n",
        "\n",
        "    model = tf.keras.models.Model(inputs=[input_ids, attention_mask, decoder_input_ids],\n",
        "                                  outputs=[t5_logits])\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
        "                  loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                  metrics=['accuracy'])\n",
        "    \n",
        "    return model"
      ],
      "metadata": {
        "id": "ZacUWKNwrMcU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_wrapper = build_t5_training_wrapper_model(t5_model_tf, max_length)"
      ],
      "metadata": {
        "id": "ZXRCuarjrMen"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# As in the first notebook, we should add a model checkpoint callback to save\n",
        "# the trained model weights after each epoch. Edit the filepath to where\n",
        "# you want to save the weights in your own Drive\n",
        "\n",
        "checkpoint_dir = 'drive/MyDrive/ISchool/MIDS/W266/2022_Fall/notebooks/model_checkpoints/'\n",
        "checkpoint_filepath = checkpoint_dir + 't5_shakespeare_weights.{epoch:02d}-{val_accuracy:.2f}.hdf5'\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_weights_only=True)"
      ],
      "metadata": {
        "id": "VDh-uMYG8jLe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now call .fit on the model_wrapper, passing in the data generators and the\n",
        "# model checkpoint callback\n",
        "\n",
        "model_wrapper.fit(train_data_generator,\n",
        "                  validation_data=valid_data_generator,\n",
        "                  epochs=1,\n",
        "                  callbacks=[model_checkpoint_callback])"
      ],
      "metadata": {
        "id": "5KkhVMODrMg9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Does it work?\n",
        "\n",
        "Again, depending on your task, you'll add your own model evaluation after training. Here's a simple check to make sure it does seem to have fine-tuned T5 for this new task we defined."
      ],
      "metadata": {
        "id": "D9NGPOrhUbRC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for test_input_text in ['Hence forth thou shalt not vex me e\\'er again.',\n",
        "                        'Dost thou foresake me?',\n",
        "                        'Makest thine own dinner.']:\n",
        "    test_inputs = t5_tokenizer([prefix + test_input_text], return_tensors='pt')\n",
        "    test_output_ids = t5_model_tf.generate(test_inputs['input_ids'])\n",
        "\n",
        "    print([t5_tokenizer.decode(out_ids, skip_special_tokens=True, \n",
        "                              clean_up_tokenization_spaces=False) for out_ids in test_output_ids])"
      ],
      "metadata": {
        "id": "5ky4-uaxGweg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To pick back up where you left off, load the saved model weights\n",
        "# (Edit the filename to the last saved one that you want to load)\n",
        "\n",
        "checkpoint_filepath = checkpoint_dir + 't5_shakespeare_weights.01-0.85.hdf5'\n",
        "model_wrapper.load_weights(checkpoint_filepath)"
      ],
      "metadata": {
        "id": "GzuZEB6EUa7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Still works?\n",
        "for test_input_text in ['Hence forth thou shalt not vex me e\\'er again.',\n",
        "                        'Dost thou foresake me?',\n",
        "                        'Makest thine own dinner.']:\n",
        "    test_inputs = t5_tokenizer([prefix + test_input_text], return_tensors='pt')\n",
        "    test_output_ids = t5_model_tf.generate(test_inputs['input_ids'])\n",
        "\n",
        "    print([t5_tokenizer.decode(out_ids, skip_special_tokens=True, \n",
        "                              clean_up_tokenization_spaces=False) for out_ids in test_output_ids])"
      ],
      "metadata": {
        "id": "qFxYoN-kC133"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "55l1GRNtC5P8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}