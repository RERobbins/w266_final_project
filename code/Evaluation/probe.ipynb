{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeanlucjackson/w266_final_project/blob/main/code/Evaluation/probe.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "c3906UT0dyke",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a555c9a-5c9f-49cc-c877-b3322ca94f00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 5.5 MB 5.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 182 kB 37.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 24.6 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "vBX23WXdd0v-"
      },
      "outputs": [],
      "source": [
        "!pip install -q sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "\n",
        "from transformers import BartTokenizer, BartForConditionalGeneration\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n"
      ],
      "metadata": {
        "id": "0RC42_aYMGDc"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "17N1XaF7d6dW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0f56e4d-ff98-4150-9557-da6cd54936d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rlCLzceMMahp"
      },
      "outputs": [],
      "source": [
        "# Set these constants for each model and validation dataset combination\n",
        "\n",
        "model_name = \"bart_base_pt_long.nq\"\n",
        "validation_dataset_names = [\"nq\", \"quac\", \"squad\", \"triviaqa\"]\n",
        "\n",
        "save_predictions = True\n",
        "save_mode = 'w' # w for write, a for append\n",
        "\n",
        "max_length = 1024 # 1024 for long model and 512 otherwise\n",
        "batch_size = 50\n",
        "\n",
        "start_sample = None  # If None, then 0 will be used\n",
        "end_sample = None # If None, then the end of the set will be used"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hh-4vVXTMhpC"
      },
      "source": [
        "### Generate Predictions From An Awesome Validation Dataset\n",
        "\n",
        "This notebook assumes a T5 PyTorch model.\n",
        "\n",
        "Setting the constants in the next call should be all that is necessary to run the validation set."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Probe Class"
      ],
      "metadata": {
        "id": "2yLqiUSeMG_M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Probe:\n",
        "\n",
        "  project_root = \"/content/drive/MyDrive/w266 NLP Final Project/\"\n",
        "  model_root = project_root + \"Models/\"\n",
        "\n",
        "  def __init__ (self):\n",
        "    self.models = {}\n",
        "    self.tokenizers = {}\n",
        "\n",
        "  def predict(self, context, question, \n",
        "              base_model='bart', training_dataset='amalgam', \n",
        "              num_beams=1, early_stopping=False, no_repeat_ngram_size=0, \n",
        "              maximum_input_length = 1024, maximum_target_length = 50):\n",
        "    \n",
        "    tokenizer = self.retrieve_tokenizer(base_model)\n",
        "    model = self.retrieve_model(base_mode, training_dataset)\n",
        "    return\n",
        "\n",
        "  def retrieve_tokenizer(self, base_model='bart'):\n",
        "    tokenizer = self.tokenizers.get(base_model)\n",
        "    if tokenizer:\n",
        "      return tokenizer\n",
        "\n",
        "    if base_model=='bart':\n",
        "      tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n",
        "    elif base_model==\"T5\":\n",
        "      tokenizer = T5Tokenizer.from_pretrained(\"google/t5-v1_1-base\")\n",
        "    else:\n",
        "      raise ValueError (\"invalid base model\")\n",
        "\n",
        "    self.tokenizers[base_model] = tokenizer\n",
        "    return tokenizer\n",
        "\n",
        "  def retrieve_model(self, base_model='bart', training_dataset='amalgam'):\n",
        "    model_tuple = (base_model, training_dataset)\n",
        "    model = self.models.get(model_tuple)\n",
        "    if model:\n",
        "      return model\n",
        "    \n",
        "    model_dir=f\"{self.model_root}{base_model}_base_pt_long.{training_dataset}\"\n",
        "\n",
        "    if base_model=='bart':\n",
        "      model = BartForConditionalGeneration.from_pretrained(model_dir)\n",
        "    elif base_model=='T5':\n",
        "      model = T5ForConditionalGeneration.from_pretrained(model_dir)\n",
        "    else:\n",
        "      raise ValueError (\"invalid base model\")\n",
        "    \n",
        "    model.to(torch.device('cuda:0'))\n",
        "    self.models[model_tuple] = model\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "vhw3l992MKIT"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p = Probe()"
      ],
      "metadata": {
        "id": "0N8OTycDYjWP"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p.retrieve_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "ywP7MgsGYpA-",
        "outputId": "b8cafcfa-c357-4872-b48d-0e1614477dd8"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-55-cf2f9222dee9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-53-e895b63b6ee0>\u001b[0m in \u001b[0;36mretrieve_model\u001b[0;34m(self, base_model, training_dataset)\u001b[0m\n\u001b[1;32m     47\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"invalid base model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda:0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_tuple\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    925\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 927\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    928\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m     def register_backward_hook(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    600\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    923\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[1;32m    924\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[0;32m--> 925\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    926\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    927\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;31m# This function throws if there's a driver initialization error, no GPUs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;31m# are found or any other error occurs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: No CUDA GPUs are available"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJ8lW8X8NJ9o",
        "tags": []
      },
      "source": [
        "### Generate Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iMHoUiDnd9He"
      },
      "outputs": [],
      "source": [
        "# Some important file locations and constants\n",
        "\n",
        "#project_root = \"/content/drive/MyDrive/w266 NLP Final Project/\"\n",
        "project_root = \"/home/localadmin/Documents/w266_NLP_Final_Project/\"\n",
        "dataset_root = project_root + \"Data/\"\n",
        "model_root = project_root + \"Models/\"\n",
        "prediction_folder = project_root + \"Predictions/checkpoint/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fEhqOalVycI9"
      },
      "outputs": [],
      "source": [
        "# Get the model and tokenizer\n",
        "\n",
        "bart_tokenizer = BartTokenizer.from_pretrained(tokenizer)\n",
        "bart_model = BartForConditionalGeneration.from_pretrained(model_folder)\n",
        "bart_model.to(torch.device('cuda:0'))\n",
        "pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zYAIRZMxwXNn",
        "outputId": "0397e222-e961-4a6a-b770-2d89ce7eb310"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/localadmin/Documents/w266_NLP_Final_Project/Data/nq/bart_valid_pairs.csv\n",
            "Generating predictions using nq from 0 to 2356:\n",
            "50 100 150 200 250 300 350 400 450 500 550 600 650 700 750 800 850 900 950 1000 \n",
            "1050 1100 1150 1200 1250 1300 1350 1400 1450 1500 1550 1600 1650 1700 1750 1800 1850 1900 1950 2000 \n",
            "2050 2100 2150 2200 2250 2300 2350 2356 \n",
            "Predictions generated.\n",
            "Write: /home/localadmin/Documents/w266_NLP_Final_Project/Predictions/checkpoint/predictions.bart_base_pt_long.nq.nq.beams.csv\n",
            "/home/localadmin/Documents/w266_NLP_Final_Project/Data/quac/bart_valid_pairs.csv\n",
            "Generating predictions using quac from 0 to 5868:\n",
            "50 100 150 200 250 300 350 400 450 500 550 600 650 700 750 800 850 900 950 1000 \n",
            "1050 1100 1150 1200 1250 1300 1350 1400 1450 1500 1550 1600 1650 1700 1750 1800 1850 1900 1950 2000 \n",
            "2050 2100 2150 2200 2250 2300 2350 2400 2450 2500 2550 2600 2650 2700 2750 2800 2850 2900 2950 3000 \n",
            "3050 3100 3150 3200 3250 3300 3350 3400 3450 3500 3550 3600 3650 3700 3750 3800 3850 3900 3950 4000 \n",
            "4050 4100 4150 4200 4250 4300 4350 4400 4450 4500 4550 4600 4650 4700 4750 4800 4850 4900 4950 5000 \n",
            "5050 5100 5150 5200 5250 5300 5350 5400 5450 5500 5550 5600 5650 5700 5750 5800 5850 5868 \n",
            "Predictions generated.\n",
            "Write: /home/localadmin/Documents/w266_NLP_Final_Project/Predictions/checkpoint/predictions.bart_base_pt_long.nq.quac.beams.csv\n",
            "/home/localadmin/Documents/w266_NLP_Final_Project/Data/squad.hf/bart_valid_pairs.csv\n",
            "Generating predictions using squad from 0 to 10570:\n",
            "50 100 150 200 250 300 350 400 450 500 550 600 650 700 750 800 850 900 950 1000 \n",
            "1050 1100 1150 1200 1250 1300 1350 1400 1450 1500 1550 1600 1650 1700 1750 1800 1850 1900 1950 2000 \n",
            "2050 2100 2150 2200 2250 2300 2350 2400 2450 2500 2550 2600 2650 2700 2750 2800 2850 2900 2950 3000 \n",
            "3050 3100 3150 3200 3250 3300 3350 3400 3450 3500 3550 3600 3650 3700 3750 3800 3850 3900 3950 4000 \n",
            "4050 4100 4150 4200 4250 4300 4350 4400 4450 4500 4550 4600 4650 4700 4750 4800 4850 4900 4950 5000 \n",
            "5050 5100 5150 5200 5250 5300 5350 5400 5450 5500 5550 5600 5650 5700 5750 5800 5850 5900 5950 6000 \n",
            "6050 6100 6150 6200 6250 6300 6350 6400 6450 6500 6550 6600 6650 6700 6750 6800 6850 6900 6950 7000 \n",
            "7050 7100 7150 7200 7250 7300 7350 7400 7450 7500 7550 7600 7650 7700 7750 7800 7850 7900 7950 8000 \n",
            "8050 8100 8150 8200 8250 8300 8350 8400 8450 8500 8550 8600 8650 8700 8750 8800 8850 8900 8950 9000 \n",
            "9050 9100 9150 9200 9250 9300 9350 9400 9450 9500 9550 9600 9650 9700 9750 9800 9850 9900 9950 10000 \n",
            "10050 10100 10150 10200 10250 10300 10350 10400 10450 10500 10550 10570 \n",
            "Predictions generated.\n",
            "Write: /home/localadmin/Documents/w266_NLP_Final_Project/Predictions/checkpoint/predictions.bart_base_pt_long.nq.squad.beams.csv\n",
            "/home/localadmin/Documents/w266_NLP_Final_Project/Data/triviaqa/bart_valid_pairs.csv\n",
            "Generating predictions using triviaqa from 0 to 9835:\n",
            "50 100 150 200 250 300 350 400 450 500 550 600 650 700 750 800 850 900 950 1000 \n",
            "1050 1100 1150 1200 1250 1300 1350 1400 1450 1500 1550 1600 1650 1700 1750 1800 1850 1900 1950 2000 \n",
            "2050 2100 2150 2200 2250 2300 2350 2400 2450 2500 2550 2600 2650 2700 2750 2800 2850 2900 2950 3000 \n",
            "3050 3100 3150 3200 3250 3300 3350 3400 3450 3500 3550 3600 3650 3700 3750 3800 3850 3900 3950 4000 \n",
            "4050 4100 4150 4200 4250 4300 4350 4400 4450 4500 4550 4600 4650 4700 4750 4800 4850 4900 4950 5000 \n",
            "5050 5100 5150 5200 5250 5300 5350 5400 5450 5500 5550 5600 5650 5700 5750 5800 5850 5900 5950 6000 \n",
            "6050 6100 6150 6200 6250 6300 6350 6400 6450 6500 6550 6600 6650 6700 6750 6800 6850 6900 6950 7000 \n",
            "7050 7100 7150 7200 7250 7300 7350 7400 7450 7500 7550 7600 7650 7700 7750 7800 7850 7900 7950 8000 \n",
            "8050 8100 8150 8200 8250 8300 8350 8400 8450 8500 8550 8600 8650 8700 8750 8800 8850 8900 8950 9000 \n",
            "9050 9100 9150 9200 9250 9300 9350 9400 9450 9500 9550 9600 9650 9700 9750 9800 9835 \n",
            "Predictions generated.\n",
            "Write: /home/localadmin/Documents/w266_NLP_Final_Project/Predictions/checkpoint/predictions.bart_base_pt_long.nq.triviaqa.beams.csv\n"
          ]
        }
      ],
      "source": [
        "for dataset_name in validation_dataset_names:\n",
        "  if dataset_name == \"squad\":\n",
        "    validation_data_file = f\"{dataset_root}squad.hf/bart_valid_pairs.csv\"\n",
        "  else:  \n",
        "    validation_data_file = f\"{dataset_root}{dataset_name}/bart_valid_pairs.csv\"\n",
        "  print(validation_data_file)\n",
        "  validation_df = pd.read_csv(validation_data_file)\n",
        "  prediction_file = f\"{prediction_folder}predictions.{model_name}.{dataset_name}.beams.csv\" \n",
        "  \n",
        "  start_sample = None\n",
        "  end_sample = None\n",
        "\n",
        "  predictions = []\n",
        "  \n",
        "  if start_sample is None: start_sample = 0\n",
        "  if end_sample is None: end_sample = validation_df.shape[0]\n",
        "  \n",
        "  print(f\"Generating predictions using {dataset_name} from {start_sample} to {end_sample}:\")\n",
        "  for start in range (start_sample, end_sample, batch_size):\n",
        "     to = min([end_sample, start + batch_size])\n",
        "     inputs = bart_tokenizer(validation_df['orig'][start:to].to_list(), return_tensors='pt', max_length=max_length, truncation=True, padding=True)\n",
        "     output_ids = bart_model.generate(inputs['input_ids'].cuda(), max_length=50, num_beams=5, no_repeat_ngram_size=2, early_stopping=True)\n",
        "     prediction_batch = bart_tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
        "     predictions.extend(prediction_batch)\n",
        "     print (f\"{to} \", end=\"\")\n",
        "     if to%1000 == 0: print()\n",
        "  print(\"\\nPredictions generated.\")\n",
        "\n",
        "  df=pd.DataFrame()\n",
        "  df['context'] = [str.split('</s>')[1] for str in validation_df['orig'][start_sample:end_sample]]\n",
        "  df['answer'] =  [str.split('</s>')[0] for str in validation_df['orig'][start_sample:end_sample]]\n",
        "  df['target'] = validation_df['target']\n",
        "  df['prediction'] = predictions\n",
        "\n",
        "  if save_predictions:\n",
        "    df.to_csv(prediction_file, mode=save_mode)\n",
        "    print(f\"Write: {prediction_file}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python [conda env:gpu]",
      "language": "python",
      "name": "conda-env-gpu-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}